{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:13, 12.6MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 16:\n",
      "Image - Min Value: 2 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naEzQA0Bjgh4AGhP0ANCYoAeAxv43zXxmhIsKBtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46e91f8b00>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 16\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    maxX = 255\n",
    "    return (x/maxX)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    numLabels = 10\n",
    "    res = np.zeros( (len(x), numLabels) )\n",
    "    res[np.arange(len(x)),x] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(name=\"x\", shape=(None,image_shape[0],image_shape[1],image_shape[2]), dtype=tf.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(name=\"y\",shape=[None,n_classes], dtype=tf.float32)\n",
    "    return y\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(name=\"keep_prob\",shape=None, dtype=tf.float32)\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    F_W = tf.Variable(tf.truncated_normal( [conv_ksize[0], conv_ksize[1], int(x_tensor.shape[3]), conv_num_outputs], stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(conv_num_outputs, dtype=tf.float32))\n",
    "    padding = \"SAME\"\n",
    "    conv2d = tf.nn.conv2d(input=x_tensor, filter=F_W, strides=[1,*conv_strides,1], padding=padding) + F_b\n",
    "    conv2d  = tf.nn.bias_add(conv2d , F_b)\n",
    "   # conv2d =  tf.nn.relu(conv2d)\n",
    "    pooling = tf.nn.max_pool(conv2d, ksize=[1,pool_ksize[0],pool_ksize[1],1], padding=padding, strides=[1,*pool_strides,1])\n",
    "    return pooling \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    new_dim = int(np.prod(x_tensor.shape[1:]))\n",
    "    new_tensor = tf.reshape(tensor=x_tensor, shape=[tf.shape(x_tensor)[0],new_dim])\n",
    "    return new_tensor\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    F_W = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], stddev=0.01))\n",
    "    F_b = tf.Variable(tf.zeros(num_outputs, dtype=tf.float32))\n",
    "\n",
    "    output = tf.add(tf.matmul(x_tensor,F_W),F_b)\n",
    "\n",
    "    return output\n",
    "    # tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv_num_outputs = 10\n",
    "    #x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 32, (2,2), (1,1), (2,2), (2,2))\n",
    "    x = conv2d_maxpool(x, 64, (2,2), (1,1), (4,4), (2,2))\n",
    "    x = conv2d_maxpool(x, 128, (4,4), (1,1), (4,4), (2,2))\n",
    "    \n",
    "    \n",
    "    #tf.layers.dropout(inputs, rate=1-keep_prob[1])\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    x = flatten(x)\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    x = fully_conn(x, 2048)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 1024)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    \n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, \n",
    "                                        y:label_batch,\n",
    "                                        keep_prob:1.0}) \n",
    "    acc = session.run(accuracy, \n",
    "                feed_dict={x:valid_features, \n",
    "                           y:valid_labels, \n",
    "                           keep_prob:1.0})\n",
    "    print(\"loss: \",loss,\" validationAccuracy: \",acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 2048\n",
    "keep_probability = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  2.83562  validationAccuracy:  0.0988\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  2.10814  validationAccuracy:  0.1794\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  1.95197  validationAccuracy:  0.2732\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  1.83636  validationAccuracy:  0.341\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  1.7087  validationAccuracy:  0.379\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  1.65508  validationAccuracy:  0.3826\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  1.5673  validationAccuracy:  0.4102\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  1.58958  validationAccuracy:  0.3728\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  1.51155  validationAccuracy:  0.433\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  1.43739  validationAccuracy:  0.4452\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  1.35288  validationAccuracy:  0.473\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  1.2943  validationAccuracy:  0.4812\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  1.23001  validationAccuracy:  0.4822\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  1.25116  validationAccuracy:  0.4722\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  1.22595  validationAccuracy:  0.4814\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  1.10473  validationAccuracy:  0.519\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  1.01786  validationAccuracy:  0.5464\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  1.08878  validationAccuracy:  0.4946\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:  1.02794  validationAccuracy:  0.526\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:  0.917211  validationAccuracy:  0.5454\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:  0.85249  validationAccuracy:  0.5596\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:  0.799428  validationAccuracy:  0.5738\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:  0.756626  validationAccuracy:  0.5646\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:  0.657531  validationAccuracy:  0.5802\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:  0.55921  validationAccuracy:  0.5928\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:  0.570285  validationAccuracy:  0.576\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:  0.532893  validationAccuracy:  0.5792\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:  0.430614  validationAccuracy:  0.5964\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:  0.421012  validationAccuracy:  0.585\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:  0.449717  validationAccuracy:  0.5614\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:  0.390672  validationAccuracy:  0.5758\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:  0.317609  validationAccuracy:  0.5934\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:  0.297646  validationAccuracy:  0.5766\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:  0.230858  validationAccuracy:  0.6078\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:  0.285123  validationAccuracy:  0.5748\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:  0.248946  validationAccuracy:  0.5708\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:  0.252355  validationAccuracy:  0.568\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:  0.176352  validationAccuracy:  0.5876\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:  0.167571  validationAccuracy:  0.5774\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:  0.188671  validationAccuracy:  0.5728\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:  0.206797  validationAccuracy:  0.572\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:  0.153183  validationAccuracy:  0.5624\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:  0.155807  validationAccuracy:  0.5748\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:  0.232341  validationAccuracy:  0.5472\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:  0.189828  validationAccuracy:  0.571\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:  0.144038  validationAccuracy:  0.5636\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:  0.0991108  validationAccuracy:  0.5772\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:  0.127985  validationAccuracy:  0.5522\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:  0.0971744  validationAccuracy:  0.5772\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:  0.0601492  validationAccuracy:  0.5882\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:  0.0932628  validationAccuracy:  0.5708\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:  0.110709  validationAccuracy:  0.565\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:  0.149695  validationAccuracy:  0.5502\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:  0.0646029  validationAccuracy:  0.5884\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:  0.0970862  validationAccuracy:  0.5784\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:  0.11004  validationAccuracy:  0.561\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:  0.0663743  validationAccuracy:  0.5786\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:  0.0359404  validationAccuracy:  0.582\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:  0.0551378  validationAccuracy:  0.5766\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:  0.0466891  validationAccuracy:  0.5838\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:  0.0399251  validationAccuracy:  0.5792\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:  0.0980518  validationAccuracy:  0.55\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:  0.159021  validationAccuracy:  0.546\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:  0.0904035  validationAccuracy:  0.5534\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:  0.0551192  validationAccuracy:  0.5638\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:  0.0387258  validationAccuracy:  0.5762\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:  0.0742015  validationAccuracy:  0.5548\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:  0.0869152  validationAccuracy:  0.568\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:  0.0483755  validationAccuracy:  0.5608\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:  0.0342733  validationAccuracy:  0.5742\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:  0.0225056  validationAccuracy:  0.5914\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:  0.00900725  validationAccuracy:  0.5938\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:  0.0191123  validationAccuracy:  0.5912\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:  0.0144439  validationAccuracy:  0.5926\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:  0.00483952  validationAccuracy:  0.6002\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:  0.00473881  validationAccuracy:  0.6054\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:  0.00386128  validationAccuracy:  0.6042\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:  0.00280674  validationAccuracy:  0.6008\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:  0.00212946  validationAccuracy:  0.6032\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:  0.0012913  validationAccuracy:  0.6106\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:  0.00146929  validationAccuracy:  0.609\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:  0.00114989  validationAccuracy:  0.6106\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:  0.00113757  validationAccuracy:  0.6072\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:  0.00065531  validationAccuracy:  0.611\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:  0.000849908  validationAccuracy:  0.613\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:  0.000466816  validationAccuracy:  0.6142\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:  0.000486876  validationAccuracy:  0.6138\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:  0.000433883  validationAccuracy:  0.615\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:  0.000416644  validationAccuracy:  0.612\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:  0.000300726  validationAccuracy:  0.6152\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:  0.000344325  validationAccuracy:  0.6144\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:  0.000275153  validationAccuracy:  0.6148\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:  0.000253184  validationAccuracy:  0.6136\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:  0.0002368  validationAccuracy:  0.6154\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:  0.000227154  validationAccuracy:  0.6154\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:  0.000218956  validationAccuracy:  0.6162\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:  0.000207842  validationAccuracy:  0.6174\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:  0.000198482  validationAccuracy:  0.6162\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:  0.000189035  validationAccuracy:  0.6156\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:  0.000180506  validationAccuracy:  0.6162\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:  0.000174637  validationAccuracy:  0.6168\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:  0.000169512  validationAccuracy:  0.6156\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:  0.000163014  validationAccuracy:  0.6158\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:  0.000156867  validationAccuracy:  0.6162\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:  0.000151911  validationAccuracy:  0.6158\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:  0.000147823  validationAccuracy:  0.6154\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:  0.0001442  validationAccuracy:  0.6158\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:  0.000139646  validationAccuracy:  0.6158\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:  0.000133828  validationAccuracy:  0.6158\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:  0.000128955  validationAccuracy:  0.6162\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:  0.000125637  validationAccuracy:  0.6166\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:  0.000123348  validationAccuracy:  0.6164\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:  0.000120396  validationAccuracy:  0.6172\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:  0.00011685  validationAccuracy:  0.6164\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:  0.000113667  validationAccuracy:  0.6168\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:  0.00011105  validationAccuracy:  0.6162\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:  0.000108558  validationAccuracy:  0.6162\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:  0.000105604  validationAccuracy:  0.6166\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:  0.000102515  validationAccuracy:  0.616\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:  9.98388e-05  validationAccuracy:  0.6166\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:  9.80603e-05  validationAccuracy:  0.6162\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:  9.60738e-05  validationAccuracy:  0.6166\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:  9.38981e-05  validationAccuracy:  0.6172\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:  9.17012e-05  validationAccuracy:  0.6172\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:  8.95861e-05  validationAccuracy:  0.6168\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:  8.77637e-05  validationAccuracy:  0.6164\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:  8.63595e-05  validationAccuracy:  0.6158\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:  8.42942e-05  validationAccuracy:  0.6164\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:  8.23214e-05  validationAccuracy:  0.6156\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:  8.02901e-05  validationAccuracy:  0.616\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:  7.82114e-05  validationAccuracy:  0.6164\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:  7.65245e-05  validationAccuracy:  0.6166\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:  7.49844e-05  validationAccuracy:  0.6164\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:  7.39058e-05  validationAccuracy:  0.6158\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:  7.24289e-05  validationAccuracy:  0.6162\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:  7.08623e-05  validationAccuracy:  0.6166\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:  6.93489e-05  validationAccuracy:  0.6166\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:  6.82481e-05  validationAccuracy:  0.6162\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:  6.75021e-05  validationAccuracy:  0.6156\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:  6.59519e-05  validationAccuracy:  0.6166\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:  6.44181e-05  validationAccuracy:  0.6166\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:  6.33535e-05  validationAccuracy:  0.6168\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:  6.20921e-05  validationAccuracy:  0.6166\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:  6.07488e-05  validationAccuracy:  0.6164\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:  5.9778e-05  validationAccuracy:  0.6158\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:  5.86317e-05  validationAccuracy:  0.6164\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:  5.75427e-05  validationAccuracy:  0.6166\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:  5.63607e-05  validationAccuracy:  0.616\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:  5.4965e-05  validationAccuracy:  0.6158\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:  5.40123e-05  validationAccuracy:  0.6162\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  2.14154  validationAccuracy:  0.1938\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:  2.0817  validationAccuracy:  0.237\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:  1.99329  validationAccuracy:  0.267\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:  1.84144  validationAccuracy:  0.3352\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:  1.75593  validationAccuracy:  0.3622\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  1.69624  validationAccuracy:  0.375\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:  1.57887  validationAccuracy:  0.4236\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:  1.54377  validationAccuracy:  0.4066\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:  1.44689  validationAccuracy:  0.4504\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:  1.54713  validationAccuracy:  0.4186\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  1.46802  validationAccuracy:  0.471\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:  1.35748  validationAccuracy:  0.5044\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:  1.28706  validationAccuracy:  0.5176\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:  1.27044  validationAccuracy:  0.5228\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:  1.25247  validationAccuracy:  0.5452\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  1.35311  validationAccuracy:  0.4964\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:  1.21814  validationAccuracy:  0.551\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:  1.12555  validationAccuracy:  0.5478\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:  1.11178  validationAccuracy:  0.5738\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:  1.09838  validationAccuracy:  0.5636\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  1.07557  validationAccuracy:  0.5848\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:  1.05362  validationAccuracy:  0.5872\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:  1.08969  validationAccuracy:  0.5604\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:  0.974108  validationAccuracy:  0.6044\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:  0.982255  validationAccuracy:  0.6066\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  0.955003  validationAccuracy:  0.6278\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:  0.943305  validationAccuracy:  0.621\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:  0.94049  validationAccuracy:  0.6068\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:  0.853852  validationAccuracy:  0.6364\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:  0.864257  validationAccuracy:  0.6334\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  0.869282  validationAccuracy:  0.6446\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:  0.828859  validationAccuracy:  0.6416\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:  0.762615  validationAccuracy:  0.6568\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:  0.981683  validationAccuracy:  0.5852\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:  0.86349  validationAccuracy:  0.6164\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  0.797737  validationAccuracy:  0.655\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:  0.788078  validationAccuracy:  0.6438\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:  0.685998  validationAccuracy:  0.6662\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:  0.690829  validationAccuracy:  0.6576\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:  0.655355  validationAccuracy:  0.6606\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  0.679025  validationAccuracy:  0.67\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:  0.682754  validationAccuracy:  0.662\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:  0.623511  validationAccuracy:  0.6582\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:  0.60257  validationAccuracy:  0.6722\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:  0.564725  validationAccuracy:  0.6834\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  0.628107  validationAccuracy:  0.6648\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:  0.582805  validationAccuracy:  0.672\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:  0.549237  validationAccuracy:  0.6784\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:  0.549503  validationAccuracy:  0.6746\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:  0.490088  validationAccuracy:  0.6982\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  0.562373  validationAccuracy:  0.6698\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:  0.589919  validationAccuracy:  0.6452\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:  0.54919  validationAccuracy:  0.6552\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:  0.533112  validationAccuracy:  0.6716\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:  0.480732  validationAccuracy:  0.6866\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  0.49732  validationAccuracy:  0.6928\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:  0.475436  validationAccuracy:  0.6768\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:  0.453185  validationAccuracy:  0.6824\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:  0.425692  validationAccuracy:  0.6852\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:  0.381782  validationAccuracy:  0.6912\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  0.406311  validationAccuracy:  0.6854\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:  0.500726  validationAccuracy:  0.6386\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:  0.413614  validationAccuracy:  0.677\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:  0.38143  validationAccuracy:  0.6846\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:  0.3534  validationAccuracy:  0.6874\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  0.433586  validationAccuracy:  0.6528\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:  0.455846  validationAccuracy:  0.6562\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:  0.38609  validationAccuracy:  0.6858\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:  0.377395  validationAccuracy:  0.678\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:  0.34844  validationAccuracy:  0.6878\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  0.345804  validationAccuracy:  0.6868\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:  0.347909  validationAccuracy:  0.6776\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:  0.325499  validationAccuracy:  0.678\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:  0.319088  validationAccuracy:  0.6774\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:  0.280287  validationAccuracy:  0.6794\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  0.348964  validationAccuracy:  0.6642\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:  0.355591  validationAccuracy:  0.6628\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:  0.277975  validationAccuracy:  0.6892\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:  0.280843  validationAccuracy:  0.6798\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:  0.258118  validationAccuracy:  0.6864\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  0.26919  validationAccuracy:  0.685\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:  0.277126  validationAccuracy:  0.6878\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:  0.259673  validationAccuracy:  0.678\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:  0.291663  validationAccuracy:  0.6714\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:  0.203328  validationAccuracy:  0.6958\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  0.240099  validationAccuracy:  0.6848\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:  0.284779  validationAccuracy:  0.6756\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:  0.226674  validationAccuracy:  0.684\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:  0.237596  validationAccuracy:  0.679\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:  0.195225  validationAccuracy:  0.6846\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:  0.240516  validationAccuracy:  0.683\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:  0.237665  validationAccuracy:  0.6814\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:  0.220522  validationAccuracy:  0.6794\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:  0.222715  validationAccuracy:  0.6814\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:  0.212797  validationAccuracy:  0.6668\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:  0.251734  validationAccuracy:  0.6696\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:  0.29932  validationAccuracy:  0.6592\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:  0.236169  validationAccuracy:  0.6646\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:  0.24744  validationAccuracy:  0.6766\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:  0.220378  validationAccuracy:  0.6778\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:  0.228206  validationAccuracy:  0.6828\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:  0.263903  validationAccuracy:  0.6714\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:  0.250653  validationAccuracy:  0.6598\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:  0.214537  validationAccuracy:  0.6812\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:  0.198206  validationAccuracy:  0.6822\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:  0.21149  validationAccuracy:  0.6878\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:  0.173123  validationAccuracy:  0.6918\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:  0.190642  validationAccuracy:  0.6694\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:  0.212893  validationAccuracy:  0.6622\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:  0.201605  validationAccuracy:  0.6682\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:  0.179163  validationAccuracy:  0.6892\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:  0.157975  validationAccuracy:  0.6952\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:  0.189391  validationAccuracy:  0.671\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:  0.160191  validationAccuracy:  0.6846\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:  0.164723  validationAccuracy:  0.6758\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:  0.192898  validationAccuracy:  0.6938\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:  0.167885  validationAccuracy:  0.6856\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:  0.163014  validationAccuracy:  0.6792\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:  0.188503  validationAccuracy:  0.6604\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:  0.140344  validationAccuracy:  0.6802\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:  0.151415  validationAccuracy:  0.7012\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:  0.136787  validationAccuracy:  0.6904\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:  0.141111  validationAccuracy:  0.6892\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:  0.155909  validationAccuracy:  0.6792\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:  0.169991  validationAccuracy:  0.6682\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:  0.137203  validationAccuracy:  0.6976\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:  0.142362  validationAccuracy:  0.681\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:  0.146851  validationAccuracy:  0.6728\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:  0.12185  validationAccuracy:  0.676\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:  0.17211  validationAccuracy:  0.6564\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:  0.264077  validationAccuracy:  0.6558\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:  0.187196  validationAccuracy:  0.663\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:  0.149075  validationAccuracy:  0.6744\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:  0.144734  validationAccuracy:  0.6738\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:  0.169511  validationAccuracy:  0.6604\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:  0.185571  validationAccuracy:  0.6658\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:  0.180261  validationAccuracy:  0.68\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:  0.150377  validationAccuracy:  0.6862\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:  0.142427  validationAccuracy:  0.6876\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:  0.141304  validationAccuracy:  0.685\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:  0.212647  validationAccuracy:  0.6502\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:  0.164144  validationAccuracy:  0.661\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:  0.154045  validationAccuracy:  0.6718\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:  0.135063  validationAccuracy:  0.6776\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:  0.10744  validationAccuracy:  0.6838\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:  0.115159  validationAccuracy:  0.673\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:  0.125122  validationAccuracy:  0.6806\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:  0.149701  validationAccuracy:  0.6648\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:  0.127828  validationAccuracy:  0.6678\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:  0.103421  validationAccuracy:  0.683\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:  0.101679  validationAccuracy:  0.6884\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:  0.152621  validationAccuracy:  0.666\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:  0.117195  validationAccuracy:  0.6836\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:  0.0830671  validationAccuracy:  0.6864\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:  0.0993657  validationAccuracy:  0.6712\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:  0.111541  validationAccuracy:  0.6754\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:  0.164169  validationAccuracy:  0.6618\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:  0.0938537  validationAccuracy:  0.687\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:  0.104047  validationAccuracy:  0.682\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:  0.0943929  validationAccuracy:  0.6724\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:  0.0764266  validationAccuracy:  0.6758\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:  0.0863358  validationAccuracy:  0.6874\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:  0.110795  validationAccuracy:  0.6618\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:  0.129549  validationAccuracy:  0.6582\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:  0.132084  validationAccuracy:  0.6622\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:  0.114199  validationAccuracy:  0.6702\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:  0.118095  validationAccuracy:  0.66\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:  0.14651  validationAccuracy:  0.6498\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:  0.132872  validationAccuracy:  0.6598\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:  0.0969934  validationAccuracy:  0.6746\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:  0.0843681  validationAccuracy:  0.687\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:  0.116963  validationAccuracy:  0.6676\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:  0.0729142  validationAccuracy:  0.6878\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:  0.136132  validationAccuracy:  0.6492\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:  0.111574  validationAccuracy:  0.6766\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:  0.0915056  validationAccuracy:  0.6858\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:  0.0835748  validationAccuracy:  0.6864\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:  0.0729193  validationAccuracy:  0.6802\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:  0.0643532  validationAccuracy:  0.6846\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:  0.0959985  validationAccuracy:  0.6702\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:  0.0893469  validationAccuracy:  0.679\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:  0.0681998  validationAccuracy:  0.6784\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:  0.0554872  validationAccuracy:  0.6846\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:  0.0425337  validationAccuracy:  0.6868\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:  0.0815739  validationAccuracy:  0.6696\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:  0.0613729  validationAccuracy:  0.6752\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:  0.0876479  validationAccuracy:  0.654\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:  0.0736805  validationAccuracy:  0.6726\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:  0.0544749  validationAccuracy:  0.6678\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:  0.0823676  validationAccuracy:  0.6634\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:  0.0817823  validationAccuracy:  0.66\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:  0.0628066  validationAccuracy:  0.6668\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:  0.0897238  validationAccuracy:  0.668\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:  0.0827274  validationAccuracy:  0.6698\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:  0.0705717  validationAccuracy:  0.6714\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:  0.0973227  validationAccuracy:  0.6654\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:  0.0832087  validationAccuracy:  0.6676\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:  0.0967644  validationAccuracy:  0.6634\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:  0.0644757  validationAccuracy:  0.6654\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:  0.072893  validationAccuracy:  0.6596\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:  0.0798073  validationAccuracy:  0.6668\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:  0.0831099  validationAccuracy:  0.6846\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:  0.0778452  validationAccuracy:  0.6712\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:  0.127679  validationAccuracy:  0.6508\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:  0.103415  validationAccuracy:  0.6608\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:  0.0885373  validationAccuracy:  0.6634\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:  0.112364  validationAccuracy:  0.6762\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:  0.0593542  validationAccuracy:  0.6826\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:  0.0588803  validationAccuracy:  0.6892\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:  0.077405  validationAccuracy:  0.6672\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:  0.0445955  validationAccuracy:  0.6806\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:  0.0965781  validationAccuracy:  0.6656\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:  0.0599517  validationAccuracy:  0.6716\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:  0.069751  validationAccuracy:  0.669\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:  0.0600143  validationAccuracy:  0.6714\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:  0.103704  validationAccuracy:  0.651\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:  0.0865866  validationAccuracy:  0.6684\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:  0.0928647  validationAccuracy:  0.6588\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:  0.126655  validationAccuracy:  0.6544\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:  0.110616  validationAccuracy:  0.6612\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:  0.086537  validationAccuracy:  0.6602\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:  0.119316  validationAccuracy:  0.6604\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:  0.127886  validationAccuracy:  0.656\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:  0.0594063  validationAccuracy:  0.6734\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:  0.0438834  validationAccuracy:  0.6814\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:  0.06876  validationAccuracy:  0.6694\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:  0.0503379  validationAccuracy:  0.6772\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:  0.0492687  validationAccuracy:  0.6716\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:  0.0617866  validationAccuracy:  0.6786\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:  0.0642627  validationAccuracy:  0.6772\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:  0.0887276  validationAccuracy:  0.6616\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:  0.0643886  validationAccuracy:  0.678\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:  0.0520977  validationAccuracy:  0.6628\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:  0.0619801  validationAccuracy:  0.67\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:  0.032087  validationAccuracy:  0.6858\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:  0.0529065  validationAccuracy:  0.6674\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:  0.036013  validationAccuracy:  0.6782\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:  0.063736  validationAccuracy:  0.6718\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:  0.0562057  validationAccuracy:  0.6686\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:  0.0528918  validationAccuracy:  0.668\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:  0.0631468  validationAccuracy:  0.651\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:  0.0926953  validationAccuracy:  0.6616\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:  0.0627314  validationAccuracy:  0.6656\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:  0.0635509  validationAccuracy:  0.6598\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:  0.0428835  validationAccuracy:  0.6688\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:  0.0492744  validationAccuracy:  0.6626\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:  0.0757248  validationAccuracy:  0.6626\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:  0.0506376  validationAccuracy:  0.6738\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:  0.0603772  validationAccuracy:  0.666\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:  0.0680423  validationAccuracy:  0.669\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:  0.0497823  validationAccuracy:  0.673\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:  0.0728103  validationAccuracy:  0.6574\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:  0.0397855  validationAccuracy:  0.6792\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:  0.0403306  validationAccuracy:  0.6788\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:  0.0457062  validationAccuracy:  0.676\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:  0.0454867  validationAccuracy:  0.6714\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:  0.0472287  validationAccuracy:  0.6734\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:  0.0843154  validationAccuracy:  0.6526\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:  0.0720844  validationAccuracy:  0.654\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:  0.0722125  validationAccuracy:  0.6604\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:  0.0751227  validationAccuracy:  0.6588\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:  0.0412081  validationAccuracy:  0.6708\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:  0.0450203  validationAccuracy:  0.6638\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:  0.0493234  validationAccuracy:  0.6598\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:  0.0449435  validationAccuracy:  0.6772\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:  0.0416459  validationAccuracy:  0.6716\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:  0.0476504  validationAccuracy:  0.663\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:  0.0511263  validationAccuracy:  0.657\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:  0.0748639  validationAccuracy:  0.6498\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:  0.0604447  validationAccuracy:  0.6706\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:  0.062004  validationAccuracy:  0.6696\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:  0.0512391  validationAccuracy:  0.6602\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:  0.0403912  validationAccuracy:  0.67\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:  0.0298415  validationAccuracy:  0.6676\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:  0.0555803  validationAccuracy:  0.666\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:  0.0679751  validationAccuracy:  0.667\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:  0.0637385  validationAccuracy:  0.6592\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:  0.0549753  validationAccuracy:  0.671\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:  0.0496845  validationAccuracy:  0.6554\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:  0.0522259  validationAccuracy:  0.6704\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:  0.0253883  validationAccuracy:  0.6794\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:  0.0398193  validationAccuracy:  0.6686\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:  0.035798  validationAccuracy:  0.6682\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:  0.0273256  validationAccuracy:  0.6782\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:  0.0467045  validationAccuracy:  0.6692\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:  0.0434978  validationAccuracy:  0.6712\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:  0.0556382  validationAccuracy:  0.6584\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:  0.0822902  validationAccuracy:  0.649\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:  0.0482081  validationAccuracy:  0.6716\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:  0.030518  validationAccuracy:  0.6818\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:  0.0215182  validationAccuracy:  0.6824\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:  0.0199568  validationAccuracy:  0.6792\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:  0.0279846  validationAccuracy:  0.6726\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:  0.0337519  validationAccuracy:  0.668\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:  0.0300162  validationAccuracy:  0.672\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:  0.0332016  validationAccuracy:  0.6664\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:  0.022839  validationAccuracy:  0.682\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:  0.0192782  validationAccuracy:  0.6866\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:  0.0242111  validationAccuracy:  0.6746\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:  0.0508093  validationAccuracy:  0.6604\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:  0.0538784  validationAccuracy:  0.6616\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:  0.0326773  validationAccuracy:  0.662\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:  0.0341505  validationAccuracy:  0.6756\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:  0.021615  validationAccuracy:  0.6792\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:  0.026658  validationAccuracy:  0.6794\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:  0.0274709  validationAccuracy:  0.6642\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:  0.0534356  validationAccuracy:  0.6562\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:  0.0245239  validationAccuracy:  0.6694\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:  0.0309632  validationAccuracy:  0.6754\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:  0.0375712  validationAccuracy:  0.67\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:  0.0545244  validationAccuracy:  0.645\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:  0.0208725  validationAccuracy:  0.6642\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:  0.0365798  validationAccuracy:  0.6612\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:  0.0437749  validationAccuracy:  0.6578\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:  0.0246403  validationAccuracy:  0.6756\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:  0.0258092  validationAccuracy:  0.671\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:  0.0220212  validationAccuracy:  0.679\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:  0.0493745  validationAccuracy:  0.6622\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:  0.0319918  validationAccuracy:  0.6604\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:  0.030799  validationAccuracy:  0.6712\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:  0.0196301  validationAccuracy:  0.6702\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:  0.0145928  validationAccuracy:  0.6758\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:  0.0110537  validationAccuracy:  0.6844\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:  0.0219184  validationAccuracy:  0.67\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:  0.0251564  validationAccuracy:  0.6656\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:  0.0403041  validationAccuracy:  0.6604\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:  0.0326647  validationAccuracy:  0.6642\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:  0.0231316  validationAccuracy:  0.6796\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:  0.0178463  validationAccuracy:  0.6756\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:  0.0327314  validationAccuracy:  0.6724\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:  0.0327837  validationAccuracy:  0.6668\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:  0.0331359  validationAccuracy:  0.6778\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:  0.0213323  validationAccuracy:  0.671\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:  0.0173206  validationAccuracy:  0.6786\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:  0.0200327  validationAccuracy:  0.6716\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:  0.0294713  validationAccuracy:  0.672\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:  0.0360287  validationAccuracy:  0.6714\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:  0.0155059  validationAccuracy:  0.6712\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:  0.0315491  validationAccuracy:  0.659\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:  0.0276481  validationAccuracy:  0.6782\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:  0.0155766  validationAccuracy:  0.6734\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:  0.0202981  validationAccuracy:  0.686\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:  0.0156504  validationAccuracy:  0.671\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:  0.028738  validationAccuracy:  0.6728\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:  0.0546071  validationAccuracy:  0.667\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:  0.0278798  validationAccuracy:  0.6748\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:  0.0220765  validationAccuracy:  0.6738\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:  0.0300612  validationAccuracy:  0.6596\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:  0.0510295  validationAccuracy:  0.6644\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:  0.0227662  validationAccuracy:  0.674\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:  0.0158932  validationAccuracy:  0.6744\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:  0.0161305  validationAccuracy:  0.6682\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:  0.0247527  validationAccuracy:  0.6634\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:  0.0142264  validationAccuracy:  0.6714\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:  0.0222482  validationAccuracy:  0.68\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:  0.0396123  validationAccuracy:  0.6734\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:  0.021413  validationAccuracy:  0.6796\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:  0.0196368  validationAccuracy:  0.674\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:  0.0125733  validationAccuracy:  0.6796\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:  0.0114163  validationAccuracy:  0.6766\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:  0.0169557  validationAccuracy:  0.6742\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:  0.0135242  validationAccuracy:  0.6752\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:  0.0163521  validationAccuracy:  0.6718\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:  0.0160578  validationAccuracy:  0.6792\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:  0.0143038  validationAccuracy:  0.673\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:  0.0117065  validationAccuracy:  0.6812\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:  0.0124132  validationAccuracy:  0.6696\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:  0.0141459  validationAccuracy:  0.6708\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:  0.0176475  validationAccuracy:  0.6616\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:  0.00887423  validationAccuracy:  0.673\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:  0.00861934  validationAccuracy:  0.6734\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:  0.00776854  validationAccuracy:  0.6764\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:  0.0184636  validationAccuracy:  0.6564\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:  0.00529129  validationAccuracy:  0.6794\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:  0.0172183  validationAccuracy:  0.6722\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:  0.0100185  validationAccuracy:  0.6712\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:  0.00986333  validationAccuracy:  0.6698\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:  0.0290479  validationAccuracy:  0.6494\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:  0.0240165  validationAccuracy:  0.6666\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:  0.0161574  validationAccuracy:  0.6628\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:  0.0100555  validationAccuracy:  0.6662\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:  0.0277839  validationAccuracy:  0.664\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:  0.0262263  validationAccuracy:  0.6646\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:  0.0243655  validationAccuracy:  0.663\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:  0.0178109  validationAccuracy:  0.6734\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:  0.0148614  validationAccuracy:  0.669\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:  0.0110598  validationAccuracy:  0.6722\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:  0.0260176  validationAccuracy:  0.665\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:  0.025062  validationAccuracy:  0.6596\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:  0.0162073  validationAccuracy:  0.6628\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:  0.0152743  validationAccuracy:  0.6684\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:  0.00564761  validationAccuracy:  0.6842\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:  0.00627761  validationAccuracy:  0.6772\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:  0.010617  validationAccuracy:  0.6702\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:  0.0220625  validationAccuracy:  0.6576\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:  0.0255723  validationAccuracy:  0.6616\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:  0.02369  validationAccuracy:  0.6738\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:  0.028957  validationAccuracy:  0.672\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:  0.0208685  validationAccuracy:  0.6732\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:  0.0350891  validationAccuracy:  0.666\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:  0.0389519  validationAccuracy:  0.6546\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:  0.0341406  validationAccuracy:  0.654\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:  0.0178331  validationAccuracy:  0.6644\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:  0.0343956  validationAccuracy:  0.6656\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:  0.0127545  validationAccuracy:  0.6784\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:  0.0224183  validationAccuracy:  0.6726\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:  0.0234072  validationAccuracy:  0.667\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:  0.0265481  validationAccuracy:  0.6742\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:  0.0276905  validationAccuracy:  0.6678\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:  0.0269969  validationAccuracy:  0.6702\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:  0.0157415  validationAccuracy:  0.6706\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:  0.0157962  validationAccuracy:  0.6694\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:  0.0157406  validationAccuracy:  0.6772\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:  0.0189293  validationAccuracy:  0.6712\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:  0.00575254  validationAccuracy:  0.674\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:  0.0157297  validationAccuracy:  0.6666\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:  0.017807  validationAccuracy:  0.658\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:  0.0294922  validationAccuracy:  0.6686\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:  0.0138827  validationAccuracy:  0.6712\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:  0.0215953  validationAccuracy:  0.6574\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:  0.0322558  validationAccuracy:  0.6544\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:  0.0274904  validationAccuracy:  0.6638\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:  0.0278594  validationAccuracy:  0.661\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:  0.0230641  validationAccuracy:  0.6608\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:  0.0260716  validationAccuracy:  0.6726\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:  0.0147003  validationAccuracy:  0.6682\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:  0.0162461  validationAccuracy:  0.6652\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:  0.0131254  validationAccuracy:  0.666\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:  0.0118507  validationAccuracy:  0.6786\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:  0.0110818  validationAccuracy:  0.6754\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:  0.0263336  validationAccuracy:  0.666\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:  0.0157915  validationAccuracy:  0.667\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:  0.014579  validationAccuracy:  0.6616\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:  0.0229341  validationAccuracy:  0.6672\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:  0.00472507  validationAccuracy:  0.673\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:  0.00837402  validationAccuracy:  0.6708\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:  0.0236682  validationAccuracy:  0.6608\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:  0.0070737  validationAccuracy:  0.672\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:  0.00978528  validationAccuracy:  0.6812\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:  0.012241  validationAccuracy:  0.6636\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:  0.012861  validationAccuracy:  0.667\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:  0.0155203  validationAccuracy:  0.6646\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:  0.00975201  validationAccuracy:  0.6794\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:  0.0277318  validationAccuracy:  0.6666\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:  0.0116006  validationAccuracy:  0.6634\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:  0.0226764  validationAccuracy:  0.66\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:  0.0329198  validationAccuracy:  0.6414\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:  0.0259575  validationAccuracy:  0.6654\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:  0.026871  validationAccuracy:  0.6608\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:  0.0122888  validationAccuracy:  0.665\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:  0.0154915  validationAccuracy:  0.664\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:  0.0226213  validationAccuracy:  0.6476\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:  0.0465196  validationAccuracy:  0.653\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:  0.0152668  validationAccuracy:  0.6656\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:  0.0244707  validationAccuracy:  0.6614\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:  0.0115074  validationAccuracy:  0.675\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:  0.0121472  validationAccuracy:  0.6704\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:  0.00908928  validationAccuracy:  0.6774\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:  0.008858  validationAccuracy:  0.6598\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:  0.0221688  validationAccuracy:  0.6754\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:  0.0157323  validationAccuracy:  0.6676\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:  0.0174779  validationAccuracy:  0.6734\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:  0.0111317  validationAccuracy:  0.6798\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:  0.00953954  validationAccuracy:  0.6798\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:  0.00683384  validationAccuracy:  0.6772\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:  0.0135694  validationAccuracy:  0.6714\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:  0.0118032  validationAccuracy:  0.674\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:  0.0101387  validationAccuracy:  0.676\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:  0.00903386  validationAccuracy:  0.6834\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:  0.0144219  validationAccuracy:  0.6748\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:  0.00952796  validationAccuracy:  0.6672\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:  0.0150392  validationAccuracy:  0.6734\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:  0.0192843  validationAccuracy:  0.6652\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:  0.00750363  validationAccuracy:  0.6712\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:  0.0063286  validationAccuracy:  0.671\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:  0.0122826  validationAccuracy:  0.673\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:  0.00656513  validationAccuracy:  0.6728\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:  0.011019  validationAccuracy:  0.673\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:  0.00870882  validationAccuracy:  0.6776\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:  0.00553383  validationAccuracy:  0.6704\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:  0.00442331  validationAccuracy:  0.6806\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:  0.00748607  validationAccuracy:  0.6736\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:  0.00721208  validationAccuracy:  0.6678\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:  0.0052933  validationAccuracy:  0.6742\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:  0.00494033  validationAccuracy:  0.6756\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:  0.00502531  validationAccuracy:  0.6798\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:  0.0186051  validationAccuracy:  0.6544\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:  0.00463988  validationAccuracy:  0.6648\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:  0.00540211  validationAccuracy:  0.6726\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:  0.0189643  validationAccuracy:  0.6676\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:  0.0102574  validationAccuracy:  0.6712\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:  0.0116924  validationAccuracy:  0.6702\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:  0.0212375  validationAccuracy:  0.6618\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:  0.0156528  validationAccuracy:  0.6658\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:  0.0203109  validationAccuracy:  0.667\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:  0.00858086  validationAccuracy:  0.6796\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:  0.0092992  validationAccuracy:  0.6712\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:  0.0154463  validationAccuracy:  0.6592\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:  0.00993222  validationAccuracy:  0.6714\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:  0.00329044  validationAccuracy:  0.68\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:  0.00767762  validationAccuracy:  0.6768\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:  0.0110891  validationAccuracy:  0.6804\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:  0.00885629  validationAccuracy:  0.6756\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:  0.0181944  validationAccuracy:  0.6652\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:  0.00952727  validationAccuracy:  0.6628\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:  0.0119093  validationAccuracy:  0.6712\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:  0.0124943  validationAccuracy:  0.6712\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:  0.0133834  validationAccuracy:  0.6604\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:  0.00498621  validationAccuracy:  0.6754\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:  0.00476873  validationAccuracy:  0.6694\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:  0.00771052  validationAccuracy:  0.6692\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:  0.00786914  validationAccuracy:  0.6784\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:  0.00441494  validationAccuracy:  0.675\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:  0.0251608  validationAccuracy:  0.6662\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:  0.00775015  validationAccuracy:  0.6612\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:  0.0149702  validationAccuracy:  0.6678\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:  0.00860302  validationAccuracy:  0.674\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:  0.0089955  validationAccuracy:  0.6768\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:  0.00798324  validationAccuracy:  0.667\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:  0.021857  validationAccuracy:  0.6572\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:  0.00871188  validationAccuracy:  0.6668\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:  0.0178344  validationAccuracy:  0.6654\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:  0.0103647  validationAccuracy:  0.6748\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:  0.011514  validationAccuracy:  0.676\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:  0.00347232  validationAccuracy:  0.672\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:  0.00623732  validationAccuracy:  0.6668\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:  0.00696753  validationAccuracy:  0.6658\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:  0.0109575  validationAccuracy:  0.673\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:  0.00549158  validationAccuracy:  0.668\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:  0.00613504  validationAccuracy:  0.6718\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:  0.00816634  validationAccuracy:  0.6774\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:  0.00610154  validationAccuracy:  0.6666\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:  0.0236866  validationAccuracy:  0.6604\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:  0.00487747  validationAccuracy:  0.6694\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:  0.00568519  validationAccuracy:  0.675\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:  0.00742368  validationAccuracy:  0.671\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:  0.00418136  validationAccuracy:  0.6716\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:  0.00299057  validationAccuracy:  0.6734\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:  0.00560524  validationAccuracy:  0.67\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:  0.00518656  validationAccuracy:  0.6734\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:  0.00442893  validationAccuracy:  0.6642\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:  0.00402928  validationAccuracy:  0.6692\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:  0.00823969  validationAccuracy:  0.664\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:  0.00226635  validationAccuracy:  0.6738\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:  0.00509951  validationAccuracy:  0.6704\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:  0.00563016  validationAccuracy:  0.6646\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:  0.00589046  validationAccuracy:  0.6642\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:  0.00443924  validationAccuracy:  0.6702\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:  0.0137358  validationAccuracy:  0.6646\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:  0.00471589  validationAccuracy:  0.6808\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:  0.00978751  validationAccuracy:  0.6624\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss:  0.0139281  validationAccuracy:  0.6608\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss:  0.0112295  validationAccuracy:  0.6546\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss:  0.00778194  validationAccuracy:  0.6748\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss:  0.0136406  validationAccuracy:  0.6726\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:  0.00672057  validationAccuracy:  0.6696\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss:  0.00642283  validationAccuracy:  0.6712\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss:  0.0060619  validationAccuracy:  0.6704\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss:  0.00490054  validationAccuracy:  0.6634\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss:  0.0131427  validationAccuracy:  0.6682\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:  0.00665136  validationAccuracy:  0.662\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss:  0.00934834  validationAccuracy:  0.6578\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss:  0.0160511  validationAccuracy:  0.6576\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss:  0.00555011  validationAccuracy:  0.6664\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss:  0.00870449  validationAccuracy:  0.6692\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:  0.00721885  validationAccuracy:  0.6676\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss:  0.00661062  validationAccuracy:  0.6686\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss:  0.00367651  validationAccuracy:  0.6792\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss:  0.00766301  validationAccuracy:  0.67\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss:  0.0128832  validationAccuracy:  0.6614\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:  0.00508669  validationAccuracy:  0.6608\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss:  0.00647882  validationAccuracy:  0.6668\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss:  0.00589471  validationAccuracy:  0.673\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss:  0.00228598  validationAccuracy:  0.6874\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss:  0.010469  validationAccuracy:  0.6746\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:  0.00847838  validationAccuracy:  0.6692\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss:  0.00561849  validationAccuracy:  0.6762\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss:  0.00577008  validationAccuracy:  0.6672\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss:  0.00432851  validationAccuracy:  0.6778\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss:  0.00408343  validationAccuracy:  0.667\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:  0.0278778  validationAccuracy:  0.6564\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss:  0.0158849  validationAccuracy:  0.661\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss:  0.00986489  validationAccuracy:  0.6636\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss:  0.00680411  validationAccuracy:  0.679\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss:  0.00429941  validationAccuracy:  0.6746\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:  0.013117  validationAccuracy:  0.662\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss:  0.00852115  validationAccuracy:  0.6538\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss:  0.0171633  validationAccuracy:  0.6672\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss:  0.0141497  validationAccuracy:  0.6702\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss:  0.00590995  validationAccuracy:  0.6712\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:  0.00267127  validationAccuracy:  0.68\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss:  0.00888521  validationAccuracy:  0.6716\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss:  0.00691563  validationAccuracy:  0.663\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss:  0.00899744  validationAccuracy:  0.6746\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss:  0.00620878  validationAccuracy:  0.6666\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:  0.0106989  validationAccuracy:  0.678\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss:  0.00633608  validationAccuracy:  0.666\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss:  0.00483966  validationAccuracy:  0.6712\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss:  0.00505508  validationAccuracy:  0.6614\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss:  0.00855738  validationAccuracy:  0.6752\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:  0.00944184  validationAccuracy:  0.6682\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss:  0.0102784  validationAccuracy:  0.6724\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss:  0.00306829  validationAccuracy:  0.6824\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss:  0.0118023  validationAccuracy:  0.6626\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss:  0.00327609  validationAccuracy:  0.6728\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:  0.00722229  validationAccuracy:  0.6658\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss:  0.00798528  validationAccuracy:  0.663\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss:  0.00494679  validationAccuracy:  0.673\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss:  0.0058541  validationAccuracy:  0.6686\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss:  0.00656744  validationAccuracy:  0.6748\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:  0.0063145  validationAccuracy:  0.6634\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss:  0.0108294  validationAccuracy:  0.667\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss:  0.00593884  validationAccuracy:  0.6652\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss:  0.00927826  validationAccuracy:  0.6614\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss:  0.0105002  validationAccuracy:  0.675\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:  0.00620426  validationAccuracy:  0.675\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss:  0.00768562  validationAccuracy:  0.6756\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss:  0.00309227  validationAccuracy:  0.6672\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss:  0.0065654  validationAccuracy:  0.6696\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss:  0.00921801  validationAccuracy:  0.666\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:  0.0286924  validationAccuracy:  0.6528\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss:  0.00598896  validationAccuracy:  0.6714\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss:  0.00843842  validationAccuracy:  0.6698\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss:  0.00295085  validationAccuracy:  0.674\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss:  0.0107036  validationAccuracy:  0.669\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:  0.00408714  validationAccuracy:  0.6704\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss:  0.00341478  validationAccuracy:  0.671\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss:  0.00672412  validationAccuracy:  0.6722\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss:  0.00603124  validationAccuracy:  0.668\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss:  0.00874976  validationAccuracy:  0.6668\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:  0.0105194  validationAccuracy:  0.6634\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss:  0.0123663  validationAccuracy:  0.6716\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss:  0.00457853  validationAccuracy:  0.6754\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss:  0.00394493  validationAccuracy:  0.6734\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss:  0.0065011  validationAccuracy:  0.6814\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:  0.0162016  validationAccuracy:  0.6614\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss:  0.0145901  validationAccuracy:  0.6772\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss:  0.0173487  validationAccuracy:  0.666\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss:  0.027384  validationAccuracy:  0.6604\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss:  0.0073218  validationAccuracy:  0.6764\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:  0.0152807  validationAccuracy:  0.6728\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss:  0.00932155  validationAccuracy:  0.672\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss:  0.0145421  validationAccuracy:  0.6724\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss:  0.00704281  validationAccuracy:  0.6714\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss:  0.00704689  validationAccuracy:  0.67\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:  0.00958244  validationAccuracy:  0.6734\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss:  0.00865366  validationAccuracy:  0.668\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss:  0.0133207  validationAccuracy:  0.6664\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss:  0.00401761  validationAccuracy:  0.6696\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss:  0.00726442  validationAccuracy:  0.6756\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:  0.00457951  validationAccuracy:  0.6774\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss:  0.00377698  validationAccuracy:  0.677\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss:  0.00603407  validationAccuracy:  0.6768\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss:  0.00706142  validationAccuracy:  0.68\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss:  0.00471732  validationAccuracy:  0.6852\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:  0.00424576  validationAccuracy:  0.6866\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss:  0.00330384  validationAccuracy:  0.6744\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss:  0.00399496  validationAccuracy:  0.6678\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss:  0.00734132  validationAccuracy:  0.6652\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss:  0.00523427  validationAccuracy:  0.678\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:  0.00725355  validationAccuracy:  0.6772\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss:  0.0013389  validationAccuracy:  0.6882\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss:  0.0126493  validationAccuracy:  0.6704\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss:  0.0132002  validationAccuracy:  0.6718\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss:  0.00302979  validationAccuracy:  0.6732\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:  0.00608413  validationAccuracy:  0.6796\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss:  0.0078911  validationAccuracy:  0.671\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss:  0.0073015  validationAccuracy:  0.681\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss:  0.0169305  validationAccuracy:  0.66\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss:  0.00938428  validationAccuracy:  0.6724\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:  0.0121832  validationAccuracy:  0.6682\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss:  0.00812935  validationAccuracy:  0.671\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss:  0.00454511  validationAccuracy:  0.6742\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss:  0.0123548  validationAccuracy:  0.6676\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss:  0.00489032  validationAccuracy:  0.6858\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:  0.0140866  validationAccuracy:  0.6788\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss:  0.0262004  validationAccuracy:  0.6576\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss:  0.00911311  validationAccuracy:  0.656\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss:  0.0231368  validationAccuracy:  0.6508\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss:  0.012061  validationAccuracy:  0.6642\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:  0.00515931  validationAccuracy:  0.6698\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss:  0.0051839  validationAccuracy:  0.6772\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss:  0.00840941  validationAccuracy:  0.6704\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss:  0.0127569  validationAccuracy:  0.6658\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss:  0.00670892  validationAccuracy:  0.6668\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:  0.00971952  validationAccuracy:  0.678\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss:  0.0171051  validationAccuracy:  0.6664\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss:  0.00861828  validationAccuracy:  0.6656\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss:  0.00428883  validationAccuracy:  0.6738\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss:  0.00435074  validationAccuracy:  0.67\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:  0.00428197  validationAccuracy:  0.6802\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss:  0.00650095  validationAccuracy:  0.6718\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss:  0.0180547  validationAccuracy:  0.662\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss:  0.0199113  validationAccuracy:  0.6516\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss:  0.00568831  validationAccuracy:  0.6706\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:  0.00825404  validationAccuracy:  0.6718\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss:  0.00499096  validationAccuracy:  0.6792\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss:  0.00751636  validationAccuracy:  0.6734\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss:  0.00991657  validationAccuracy:  0.6674\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss:  0.00910166  validationAccuracy:  0.6722\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:  0.00869962  validationAccuracy:  0.6726\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss:  0.00860523  validationAccuracy:  0.6668\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss:  0.00861724  validationAccuracy:  0.6634\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss:  0.00908473  validationAccuracy:  0.6776\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss:  0.0106003  validationAccuracy:  0.673\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:  0.00489171  validationAccuracy:  0.6722\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss:  0.00621367  validationAccuracy:  0.6648\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss:  0.00620024  validationAccuracy:  0.6682\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss:  0.0130151  validationAccuracy:  0.67\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss:  0.00785538  validationAccuracy:  0.6684\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:  0.00911477  validationAccuracy:  0.6712\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss:  0.00863311  validationAccuracy:  0.6726\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss:  0.00434121  validationAccuracy:  0.6798\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss:  0.00392175  validationAccuracy:  0.6664\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss:  0.00663882  validationAccuracy:  0.6738\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:  0.0111798  validationAccuracy:  0.6728\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss:  0.00361954  validationAccuracy:  0.6786\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss:  0.0105606  validationAccuracy:  0.6706\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss:  0.00629665  validationAccuracy:  0.663\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss:  0.0154682  validationAccuracy:  0.6664\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:  0.0075421  validationAccuracy:  0.6668\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss:  0.00412065  validationAccuracy:  0.673\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss:  0.00509898  validationAccuracy:  0.67\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss:  0.00606134  validationAccuracy:  0.6716\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss:  0.00964888  validationAccuracy:  0.6672\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:  0.00983079  validationAccuracy:  0.6646\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss:  0.00268321  validationAccuracy:  0.6758\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss:  0.00356653  validationAccuracy:  0.6782\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss:  0.0030968  validationAccuracy:  0.6722\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss:  0.00304368  validationAccuracy:  0.6614\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:  0.0103844  validationAccuracy:  0.6666\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss:  0.0108244  validationAccuracy:  0.6768\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss:  0.00993139  validationAccuracy:  0.6772\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss:  0.0101623  validationAccuracy:  0.665\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss:  0.010334  validationAccuracy:  0.6756\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:  0.00930346  validationAccuracy:  0.6596\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss:  0.0151509  validationAccuracy:  0.6586\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss:  0.0196055  validationAccuracy:  0.658\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss:  0.0129551  validationAccuracy:  0.6696\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss:  0.00362206  validationAccuracy:  0.675\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:  0.00286655  validationAccuracy:  0.6752\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss:  0.003807  validationAccuracy:  0.6652\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss:  0.00465055  validationAccuracy:  0.6596\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss:  0.00970567  validationAccuracy:  0.6694\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss:  0.00824252  validationAccuracy:  0.6712\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:  0.00919348  validationAccuracy:  0.6702\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss:  0.0119903  validationAccuracy:  0.6708\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss:  0.0121733  validationAccuracy:  0.6688\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss:  0.00711804  validationAccuracy:  0.6678\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss:  0.00244252  validationAccuracy:  0.6722\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6819284200668335\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4788aebfd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
