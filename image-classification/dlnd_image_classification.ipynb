{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 16:\n",
      "Image - Min Value: 2 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naEzQA0Bjgh4AGhP0ANCYoAeAxv43zXxmhIsKBtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3fe5cb828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 16\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    maxX = 255\n",
    "    return (x/maxX)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    numLabels = 10\n",
    "    res = np.zeros( (len(x), numLabels) )\n",
    "    res[np.arange(len(x)),x] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(name=\"x\", shape=(None,image_shape[0],image_shape[1],image_shape[2]), dtype=tf.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(name=\"y\",shape=[None,n_classes], dtype=tf.float32)\n",
    "    return y\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(name=\"keep_prob\",shape=None, dtype=tf.float32)\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    F_W = tf.Variable(tf.truncated_normal( [conv_ksize[0], conv_ksize[1], int(x_tensor.shape[3]), conv_num_outputs], stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(conv_num_outputs, dtype=tf.float32))\n",
    "    padding = \"SAME\"\n",
    "    conv2d = tf.nn.conv2d(input=x_tensor, filter=F_W, strides=[1,*conv_strides,1], padding=padding) + F_b\n",
    "    conv2d  = tf.nn.bias_add(conv2d , F_b)\n",
    "    conv2d =  tf.nn.relu(conv2d)\n",
    "    pooling = tf.nn.max_pool(conv2d, ksize=[1,pool_ksize[0],pool_ksize[1],1], padding=padding, strides=[1,*pool_strides,1])\n",
    "    return pooling \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    new_dim = int(np.prod(x_tensor.shape[1:]))\n",
    "    new_tensor = tf.reshape(tensor=x_tensor, shape=[tf.shape(x_tensor)[0],new_dim])\n",
    "    return new_tensor\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    F_W = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], stddev=0.01))\n",
    "    F_b = tf.Variable(tf.zeros(num_outputs, dtype=tf.float32))\n",
    "\n",
    "    output = tf.add(tf.matmul(x_tensor,F_W),F_b)\n",
    "    output = tf.nn.relu(output) #adding a RELU as requested\n",
    "    return output\n",
    "    # Old Code: tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv_num_outputs = 10\n",
    "    #x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 32, (2,2), (1,1), (2,2), (2,2))\n",
    "    x = conv2d_maxpool(x, 64, (2,2), (1,1), (4,4), (2,2))\n",
    "    x = conv2d_maxpool(x, 128, (4,4), (1,1), (4,4), (2,2))\n",
    "    \n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    x = flatten(x)\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    x = fully_conn(x, 2048)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 1024)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    \n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, \n",
    "                                        y:label_batch,\n",
    "                                        keep_prob:1.0}) \n",
    "    acc = session.run(accuracy, \n",
    "                feed_dict={x:valid_features, \n",
    "                           y:valid_labels, \n",
    "                           keep_prob:1.0})\n",
    "    print(\"loss: \",loss,\" validationAccuracy: \",acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 110\n",
    "batch_size = 2048\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  2.29039  validationAccuracy:  0.1822\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  2.20237  validationAccuracy:  0.1746\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  2.29875  validationAccuracy:  0.1008\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  2.27484  validationAccuracy:  0.141\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  2.1524  validationAccuracy:  0.16\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  2.10492  validationAccuracy:  0.1938\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  2.01695  validationAccuracy:  0.2566\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  1.98577  validationAccuracy:  0.2734\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  1.92338  validationAccuracy:  0.2898\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  1.90995  validationAccuracy:  0.2688\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  1.87266  validationAccuracy:  0.2794\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  1.76171  validationAccuracy:  0.332\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  1.71112  validationAccuracy:  0.3314\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  1.6683  validationAccuracy:  0.3544\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  1.58267  validationAccuracy:  0.3718\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  1.7111  validationAccuracy:  0.3466\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  1.63279  validationAccuracy:  0.3812\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  1.59762  validationAccuracy:  0.3896\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:  1.59866  validationAccuracy:  0.3688\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:  1.51266  validationAccuracy:  0.396\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:  1.47133  validationAccuracy:  0.4024\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:  1.40076  validationAccuracy:  0.4246\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:  1.35879  validationAccuracy:  0.4266\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:  1.51837  validationAccuracy:  0.3948\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:  1.37516  validationAccuracy:  0.4314\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:  1.33785  validationAccuracy:  0.4472\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:  1.30863  validationAccuracy:  0.4512\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:  1.23462  validationAccuracy:  0.4754\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:  1.21875  validationAccuracy:  0.481\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:  1.19459  validationAccuracy:  0.4834\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:  1.18651  validationAccuracy:  0.4812\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:  1.19792  validationAccuracy:  0.4878\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:  1.16155  validationAccuracy:  0.4894\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:  1.07232  validationAccuracy:  0.5032\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:  1.02946  validationAccuracy:  0.524\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:  0.98918  validationAccuracy:  0.5306\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:  1.18208  validationAccuracy:  0.478\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:  1.56035  validationAccuracy:  0.381\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:  1.38228  validationAccuracy:  0.423\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:  1.26282  validationAccuracy:  0.455\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:  1.18626  validationAccuracy:  0.49\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:  1.13932  validationAccuracy:  0.4942\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:  1.05265  validationAccuracy:  0.5124\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:  1.00263  validationAccuracy:  0.519\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:  0.957574  validationAccuracy:  0.5282\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:  0.90866  validationAccuracy:  0.538\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:  0.870819  validationAccuracy:  0.54\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:  0.890558  validationAccuracy:  0.515\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:  0.825428  validationAccuracy:  0.5498\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:  0.779396  validationAccuracy:  0.5462\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:  0.794049  validationAccuracy:  0.5512\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:  0.995998  validationAccuracy:  0.4986\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:  0.762953  validationAccuracy:  0.5596\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:  0.697751  validationAccuracy:  0.563\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:  0.657451  validationAccuracy:  0.566\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:  0.644658  validationAccuracy:  0.5672\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:  0.598388  validationAccuracy:  0.5784\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:  0.573181  validationAccuracy:  0.5706\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:  0.539982  validationAccuracy:  0.573\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:  0.56941  validationAccuracy:  0.5626\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:  0.502211  validationAccuracy:  0.5812\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:  0.467763  validationAccuracy:  0.5828\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:  0.44455  validationAccuracy:  0.5808\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:  0.401858  validationAccuracy:  0.5868\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:  0.35999  validationAccuracy:  0.5902\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:  0.410802  validationAccuracy:  0.577\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:  0.329163  validationAccuracy:  0.5942\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:  0.392369  validationAccuracy:  0.5716\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:  0.359794  validationAccuracy:  0.5676\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:  0.444775  validationAccuracy:  0.5606\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:  0.371573  validationAccuracy:  0.581\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:  0.37711  validationAccuracy:  0.5798\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:  0.387058  validationAccuracy:  0.5668\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:  0.360283  validationAccuracy:  0.5688\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:  0.321247  validationAccuracy:  0.5722\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:  0.348518  validationAccuracy:  0.5598\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:  0.395844  validationAccuracy:  0.5472\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:  0.320635  validationAccuracy:  0.5726\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:  0.501226  validationAccuracy:  0.5212\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:  0.324292  validationAccuracy:  0.5718\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:  0.291766  validationAccuracy:  0.5866\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:  0.245852  validationAccuracy:  0.5872\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:  0.222686  validationAccuracy:  0.5906\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:  0.216279  validationAccuracy:  0.5768\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:  0.178929  validationAccuracy:  0.5858\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:  0.207215  validationAccuracy:  0.5662\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:  0.377459  validationAccuracy:  0.5208\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:  0.3556  validationAccuracy:  0.544\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:  0.245606  validationAccuracy:  0.581\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:  0.215916  validationAccuracy:  0.587\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:  0.156118  validationAccuracy:  0.5916\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:  0.124326  validationAccuracy:  0.5966\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:  0.0949126  validationAccuracy:  0.5906\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:  0.0866717  validationAccuracy:  0.6004\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:  0.0714735  validationAccuracy:  0.5964\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:  0.075285  validationAccuracy:  0.5852\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:  0.0887338  validationAccuracy:  0.5824\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:  0.13196  validationAccuracy:  0.5688\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:  0.151417  validationAccuracy:  0.5474\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:  0.187104  validationAccuracy:  0.5468\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:  0.111097  validationAccuracy:  0.5772\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:  0.254549  validationAccuracy:  0.5526\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:  0.186432  validationAccuracy:  0.5666\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:  0.214906  validationAccuracy:  0.5522\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:  0.277232  validationAccuracy:  0.5496\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:  0.143415  validationAccuracy:  0.5788\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:  0.0980005  validationAccuracy:  0.594\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:  0.100786  validationAccuracy:  0.5906\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:  0.0715831  validationAccuracy:  0.5926\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:  0.0664011  validationAccuracy:  0.605\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  2.28451  validationAccuracy:  0.1908\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:  2.26808  validationAccuracy:  0.1262\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:  2.17237  validationAccuracy:  0.1644\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:  2.01642  validationAccuracy:  0.2276\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:  2.00576  validationAccuracy:  0.2302\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  1.9829  validationAccuracy:  0.2468\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:  1.85323  validationAccuracy:  0.2832\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:  2.04468  validationAccuracy:  0.2134\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:  1.83521  validationAccuracy:  0.2832\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:  1.86301  validationAccuracy:  0.2522\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  1.83888  validationAccuracy:  0.3088\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:  1.76345  validationAccuracy:  0.3044\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:  1.70257  validationAccuracy:  0.324\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:  1.66936  validationAccuracy:  0.3472\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:  1.64845  validationAccuracy:  0.3458\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  1.62718  validationAccuracy:  0.37\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:  1.61525  validationAccuracy:  0.3804\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:  1.52952  validationAccuracy:  0.3778\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:  1.53471  validationAccuracy:  0.3922\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:  1.53658  validationAccuracy:  0.4054\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  1.53919  validationAccuracy:  0.4016\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:  1.55002  validationAccuracy:  0.4242\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:  1.42713  validationAccuracy:  0.4218\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:  1.41632  validationAccuracy:  0.4422\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:  1.41519  validationAccuracy:  0.459\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  1.37018  validationAccuracy:  0.4604\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:  1.41397  validationAccuracy:  0.4618\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:  1.32185  validationAccuracy:  0.4706\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:  1.29601  validationAccuracy:  0.4788\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:  1.3221  validationAccuracy:  0.4884\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  1.26857  validationAccuracy:  0.5024\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:  1.35897  validationAccuracy:  0.4864\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:  1.30526  validationAccuracy:  0.4818\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:  1.27891  validationAccuracy:  0.483\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:  1.22833  validationAccuracy:  0.5158\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  1.19645  validationAccuracy:  0.5316\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:  1.20185  validationAccuracy:  0.5416\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:  1.2018  validationAccuracy:  0.5138\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:  1.21266  validationAccuracy:  0.5176\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:  1.16702  validationAccuracy:  0.5434\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  1.16871  validationAccuracy:  0.5448\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:  1.15631  validationAccuracy:  0.556\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:  1.11447  validationAccuracy:  0.554\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:  1.08835  validationAccuracy:  0.5546\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:  1.07936  validationAccuracy:  0.5644\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  1.08801  validationAccuracy:  0.5694\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:  1.12675  validationAccuracy:  0.5576\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:  1.09076  validationAccuracy:  0.5546\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:  1.06026  validationAccuracy:  0.5598\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:  1.0712  validationAccuracy:  0.5606\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  1.0583  validationAccuracy:  0.5776\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:  1.07586  validationAccuracy:  0.5634\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:  1.01756  validationAccuracy:  0.5786\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:  0.963692  validationAccuracy:  0.5926\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:  0.963976  validationAccuracy:  0.5866\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  0.956223  validationAccuracy:  0.5998\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:  0.969324  validationAccuracy:  0.6024\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:  0.91934  validationAccuracy:  0.6052\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:  0.942943  validationAccuracy:  0.5914\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:  0.907347  validationAccuracy:  0.598\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  0.95111  validationAccuracy:  0.6028\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:  0.936167  validationAccuracy:  0.6114\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:  0.898267  validationAccuracy:  0.6114\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:  0.841927  validationAccuracy:  0.621\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:  0.838221  validationAccuracy:  0.6218\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  0.861091  validationAccuracy:  0.6262\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:  0.8626  validationAccuracy:  0.6198\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:  0.859056  validationAccuracy:  0.6256\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:  0.818145  validationAccuracy:  0.6308\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:  0.809038  validationAccuracy:  0.6208\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  0.800872  validationAccuracy:  0.642\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:  0.793888  validationAccuracy:  0.6492\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:  0.756352  validationAccuracy:  0.6484\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:  0.785868  validationAccuracy:  0.6342\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:  0.746872  validationAccuracy:  0.6384\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  0.788199  validationAccuracy:  0.635\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:  0.780878  validationAccuracy:  0.638\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:  0.724082  validationAccuracy:  0.6556\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:  0.79923  validationAccuracy:  0.614\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:  0.723424  validationAccuracy:  0.6512\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  0.71336  validationAccuracy:  0.6546\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:  0.737444  validationAccuracy:  0.646\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:  0.69632  validationAccuracy:  0.6588\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:  0.711576  validationAccuracy:  0.6422\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:  0.703832  validationAccuracy:  0.6446\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  0.658101  validationAccuracy:  0.6702\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:  0.705416  validationAccuracy:  0.6492\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:  0.694322  validationAccuracy:  0.6538\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:  0.616979  validationAccuracy:  0.6772\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:  0.648675  validationAccuracy:  0.66\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:  0.640439  validationAccuracy:  0.6636\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:  0.603352  validationAccuracy:  0.6764\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:  0.624636  validationAccuracy:  0.668\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:  0.598246  validationAccuracy:  0.6774\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:  0.573159  validationAccuracy:  0.6764\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:  0.573443  validationAccuracy:  0.6772\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:  0.605016  validationAccuracy:  0.6688\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:  0.549955  validationAccuracy:  0.6846\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:  0.593869  validationAccuracy:  0.6664\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:  0.556475  validationAccuracy:  0.6786\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:  0.549479  validationAccuracy:  0.6858\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:  0.56859  validationAccuracy:  0.6812\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:  0.503656  validationAccuracy:  0.6844\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:  0.516958  validationAccuracy:  0.6752\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:  0.514738  validationAccuracy:  0.6788\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:  0.546371  validationAccuracy:  0.6732\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:  0.672156  validationAccuracy:  0.6452\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:  0.519445  validationAccuracy:  0.6812\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:  0.51125  validationAccuracy:  0.6836\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:  0.472883  validationAccuracy:  0.6858\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:  0.474405  validationAccuracy:  0.693\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:  0.50859  validationAccuracy:  0.6876\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:  0.506242  validationAccuracy:  0.6748\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:  0.484465  validationAccuracy:  0.6794\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:  0.447408  validationAccuracy:  0.6988\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:  0.452429  validationAccuracy:  0.6964\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:  0.462507  validationAccuracy:  0.6936\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:  0.459796  validationAccuracy:  0.686\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:  0.510129  validationAccuracy:  0.6662\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:  0.436366  validationAccuracy:  0.6826\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:  0.499008  validationAccuracy:  0.679\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:  0.490706  validationAccuracy:  0.6734\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:  0.546649  validationAccuracy:  0.6606\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:  0.536454  validationAccuracy:  0.6582\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:  0.45918  validationAccuracy:  0.6786\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:  0.473609  validationAccuracy:  0.6826\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:  0.490241  validationAccuracy:  0.6708\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:  0.464966  validationAccuracy:  0.6698\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:  0.437129  validationAccuracy:  0.6916\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:  0.385512  validationAccuracy:  0.6948\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:  0.429484  validationAccuracy:  0.6892\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:  0.428805  validationAccuracy:  0.6926\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:  0.470209  validationAccuracy:  0.6694\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:  0.394195  validationAccuracy:  0.6912\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:  0.379027  validationAccuracy:  0.6898\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:  0.385869  validationAccuracy:  0.693\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:  0.41474  validationAccuracy:  0.6834\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:  0.397476  validationAccuracy:  0.6836\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:  0.397953  validationAccuracy:  0.6804\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:  0.382024  validationAccuracy:  0.683\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:  0.377585  validationAccuracy:  0.6972\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:  0.41278  validationAccuracy:  0.681\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:  0.356541  validationAccuracy:  0.6928\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:  0.366628  validationAccuracy:  0.6862\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:  0.357951  validationAccuracy:  0.6862\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:  0.373781  validationAccuracy:  0.69\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:  0.373369  validationAccuracy:  0.69\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:  0.421242  validationAccuracy:  0.6746\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:  0.349011  validationAccuracy:  0.6982\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:  0.357843  validationAccuracy:  0.689\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:  0.333861  validationAccuracy:  0.7068\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:  0.371105  validationAccuracy:  0.6898\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:  0.308055  validationAccuracy:  0.7054\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:  0.309166  validationAccuracy:  0.6876\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:  0.265689  validationAccuracy:  0.7068\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:  0.301383  validationAccuracy:  0.6954\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:  0.315473  validationAccuracy:  0.6988\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:  0.281129  validationAccuracy:  0.7062\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:  0.284069  validationAccuracy:  0.6996\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:  0.222871  validationAccuracy:  0.7188\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:  0.281943  validationAccuracy:  0.6964\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:  0.302814  validationAccuracy:  0.6946\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:  0.298519  validationAccuracy:  0.6866\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:  0.224807  validationAccuracy:  0.7158\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:  0.217806  validationAccuracy:  0.7182\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:  0.239164  validationAccuracy:  0.7128\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:  0.244569  validationAccuracy:  0.7058\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:  0.217545  validationAccuracy:  0.7126\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:  0.203645  validationAccuracy:  0.7178\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:  0.176085  validationAccuracy:  0.7196\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:  0.210805  validationAccuracy:  0.7172\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:  0.1995  validationAccuracy:  0.719\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:  0.201568  validationAccuracy:  0.7192\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:  0.183134  validationAccuracy:  0.7136\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:  0.175046  validationAccuracy:  0.7144\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:  0.174651  validationAccuracy:  0.726\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:  0.175585  validationAccuracy:  0.7148\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:  0.218816  validationAccuracy:  0.7058\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:  0.16014  validationAccuracy:  0.7226\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:  0.145084  validationAccuracy:  0.7172\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:  0.176678  validationAccuracy:  0.7098\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:  0.232654  validationAccuracy:  0.6834\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:  0.283458  validationAccuracy:  0.68\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:  0.198221  validationAccuracy:  0.7028\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:  0.218799  validationAccuracy:  0.7064\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:  0.227359  validationAccuracy:  0.6972\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:  0.289478  validationAccuracy:  0.6786\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:  0.269761  validationAccuracy:  0.675\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:  0.263118  validationAccuracy:  0.6958\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:  0.175096  validationAccuracy:  0.7188\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:  0.179729  validationAccuracy:  0.7134\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:  0.196851  validationAccuracy:  0.7196\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:  0.160015  validationAccuracy:  0.7222\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:  0.181784  validationAccuracy:  0.6944\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:  0.116352  validationAccuracy:  0.7278\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:  0.127098  validationAccuracy:  0.7224\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:  0.168031  validationAccuracy:  0.7046\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:  0.139422  validationAccuracy:  0.7116\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:  0.140399  validationAccuracy:  0.7052\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:  0.137003  validationAccuracy:  0.7156\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:  0.142908  validationAccuracy:  0.7128\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:  0.145494  validationAccuracy:  0.7192\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:  0.145585  validationAccuracy:  0.704\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:  0.165536  validationAccuracy:  0.6906\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:  0.139727  validationAccuracy:  0.7128\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:  0.178282  validationAccuracy:  0.6956\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:  0.155699  validationAccuracy:  0.7088\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:  0.12961  validationAccuracy:  0.7186\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:  0.134896  validationAccuracy:  0.7052\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:  0.158703  validationAccuracy:  0.6816\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:  0.155029  validationAccuracy:  0.7052\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:  0.251237  validationAccuracy:  0.6758\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:  0.155767  validationAccuracy:  0.7056\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:  0.1751  validationAccuracy:  0.701\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:  0.134991  validationAccuracy:  0.695\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:  0.117534  validationAccuracy:  0.7124\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:  0.309515  validationAccuracy:  0.6332\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:  0.271327  validationAccuracy:  0.6636\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:  0.208109  validationAccuracy:  0.6926\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:  0.196605  validationAccuracy:  0.6974\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:  0.119038  validationAccuracy:  0.716\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:  0.128137  validationAccuracy:  0.7048\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:  0.117936  validationAccuracy:  0.7134\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:  0.0978743  validationAccuracy:  0.7186\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:  0.085865  validationAccuracy:  0.7188\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:  0.079602  validationAccuracy:  0.7152\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:  0.0948807  validationAccuracy:  0.703\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:  0.128395  validationAccuracy:  0.6974\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:  0.143511  validationAccuracy:  0.6902\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:  0.0864499  validationAccuracy:  0.7202\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:  0.0974636  validationAccuracy:  0.7042\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:  0.0778107  validationAccuracy:  0.723\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:  0.0881049  validationAccuracy:  0.7054\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:  0.0823973  validationAccuracy:  0.7236\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:  0.0798911  validationAccuracy:  0.7258\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:  0.0889533  validationAccuracy:  0.7118\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:  0.0879213  validationAccuracy:  0.7066\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:  0.0802483  validationAccuracy:  0.7106\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:  0.0761834  validationAccuracy:  0.7038\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:  0.0714333  validationAccuracy:  0.722\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:  0.0571141  validationAccuracy:  0.7304\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:  0.0557273  validationAccuracy:  0.7188\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:  0.0839514  validationAccuracy:  0.698\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:  0.0892909  validationAccuracy:  0.693\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:  0.06177  validationAccuracy:  0.7208\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:  0.109781  validationAccuracy:  0.703\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:  0.100283  validationAccuracy:  0.6994\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:  0.169642  validationAccuracy:  0.6756\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:  0.114675  validationAccuracy:  0.692\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:  0.128266  validationAccuracy:  0.6876\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:  0.116481  validationAccuracy:  0.7028\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:  0.108638  validationAccuracy:  0.7038\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:  0.142159  validationAccuracy:  0.6936\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:  0.0903858  validationAccuracy:  0.6958\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:  0.106942  validationAccuracy:  0.6872\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:  0.0841015  validationAccuracy:  0.7034\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:  0.132023  validationAccuracy:  0.6744\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:  0.0986048  validationAccuracy:  0.6966\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:  0.129216  validationAccuracy:  0.681\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:  0.0922892  validationAccuracy:  0.7038\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:  0.111325  validationAccuracy:  0.6844\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:  0.102553  validationAccuracy:  0.6922\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:  0.110609  validationAccuracy:  0.6894\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:  0.0935087  validationAccuracy:  0.6912\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:  0.0963242  validationAccuracy:  0.7036\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:  0.163942  validationAccuracy:  0.6586\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:  0.130476  validationAccuracy:  0.6902\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:  0.0757644  validationAccuracy:  0.7128\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:  0.0695716  validationAccuracy:  0.7138\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:  0.0874046  validationAccuracy:  0.6962\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:  0.0998309  validationAccuracy:  0.6864\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:  0.0602295  validationAccuracy:  0.705\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:  0.0514948  validationAccuracy:  0.7094\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:  0.0673302  validationAccuracy:  0.7056\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:  0.0515489  validationAccuracy:  0.7112\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:  0.0683844  validationAccuracy:  0.691\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:  0.0485045  validationAccuracy:  0.704\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:  0.0813764  validationAccuracy:  0.6886\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:  0.0513602  validationAccuracy:  0.7142\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:  0.0611505  validationAccuracy:  0.7114\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:  0.0568628  validationAccuracy:  0.7046\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:  0.0892366  validationAccuracy:  0.6866\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:  0.0537262  validationAccuracy:  0.6984\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:  0.0761336  validationAccuracy:  0.6902\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:  0.0668964  validationAccuracy:  0.711\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:  0.0368624  validationAccuracy:  0.7132\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:  0.110366  validationAccuracy:  0.6828\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:  0.0727311  validationAccuracy:  0.6874\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:  0.0648198  validationAccuracy:  0.699\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:  0.053104  validationAccuracy:  0.7102\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:  0.0700689  validationAccuracy:  0.701\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:  0.0708012  validationAccuracy:  0.696\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:  0.0879533  validationAccuracy:  0.686\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:  0.0520447  validationAccuracy:  0.7006\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:  0.0508753  validationAccuracy:  0.7022\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:  0.0564358  validationAccuracy:  0.7134\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:  0.0418383  validationAccuracy:  0.7156\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:  0.0368597  validationAccuracy:  0.705\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:  0.0845134  validationAccuracy:  0.6906\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:  0.0556923  validationAccuracy:  0.7004\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:  0.0275245  validationAccuracy:  0.7236\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:  0.0245129  validationAccuracy:  0.726\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:  0.0191488  validationAccuracy:  0.7118\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:  0.039038  validationAccuracy:  0.7006\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:  0.0383247  validationAccuracy:  0.7114\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:  0.0640047  validationAccuracy:  0.6852\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:  0.047712  validationAccuracy:  0.7136\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:  0.0409752  validationAccuracy:  0.6984\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:  0.030431  validationAccuracy:  0.7084\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:  0.0364582  validationAccuracy:  0.7102\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:  0.0218997  validationAccuracy:  0.7204\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:  0.030924  validationAccuracy:  0.7178\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:  0.0262905  validationAccuracy:  0.7104\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:  0.0171853  validationAccuracy:  0.7198\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:  0.0192825  validationAccuracy:  0.7124\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:  0.024531  validationAccuracy:  0.7136\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:  0.031805  validationAccuracy:  0.713\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:  0.0505005  validationAccuracy:  0.682\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:  0.04268  validationAccuracy:  0.6958\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:  0.029867  validationAccuracy:  0.711\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:  0.0397973  validationAccuracy:  0.705\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:  0.0270925  validationAccuracy:  0.7156\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:  0.0170852  validationAccuracy:  0.7056\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:  0.0566283  validationAccuracy:  0.7048\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:  0.04532  validationAccuracy:  0.6952\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:  0.0281416  validationAccuracy:  0.7262\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:  0.0295756  validationAccuracy:  0.7168\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:  0.0211506  validationAccuracy:  0.721\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:  0.0132451  validationAccuracy:  0.7124\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:  0.015046  validationAccuracy:  0.7152\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:  0.0169019  validationAccuracy:  0.7218\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:  0.0178252  validationAccuracy:  0.7174\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:  0.0163085  validationAccuracy:  0.7254\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:  0.0245126  validationAccuracy:  0.714\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:  0.0319971  validationAccuracy:  0.7116\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:  0.0132758  validationAccuracy:  0.7158\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:  0.0163653  validationAccuracy:  0.7238\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:  0.00986356  validationAccuracy:  0.7252\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:  0.0127445  validationAccuracy:  0.7144\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:  0.00908296  validationAccuracy:  0.715\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:  0.0147525  validationAccuracy:  0.7186\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:  0.021483  validationAccuracy:  0.7078\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:  0.00826981  validationAccuracy:  0.7224\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:  0.00657531  validationAccuracy:  0.7262\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:  0.0106202  validationAccuracy:  0.7082\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:  0.00515628  validationAccuracy:  0.7302\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:  0.00883931  validationAccuracy:  0.7174\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:  0.00779381  validationAccuracy:  0.721\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:  0.00936658  validationAccuracy:  0.7182\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:  0.00634098  validationAccuracy:  0.7136\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:  0.0101503  validationAccuracy:  0.717\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:  0.00920196  validationAccuracy:  0.7128\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:  0.013728  validationAccuracy:  0.7048\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:  0.00943731  validationAccuracy:  0.715\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:  0.00466133  validationAccuracy:  0.7168\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:  0.00631811  validationAccuracy:  0.7226\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:  0.018414  validationAccuracy:  0.7092\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:  0.0110664  validationAccuracy:  0.7114\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:  0.00857393  validationAccuracy:  0.7146\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:  0.00989906  validationAccuracy:  0.7014\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:  0.0084332  validationAccuracy:  0.716\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:  0.0130991  validationAccuracy:  0.707\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:  0.0151553  validationAccuracy:  0.7114\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:  0.0145674  validationAccuracy:  0.7086\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:  0.00745411  validationAccuracy:  0.7114\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:  0.00889589  validationAccuracy:  0.7162\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:  0.00689875  validationAccuracy:  0.7172\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:  0.00801091  validationAccuracy:  0.72\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:  0.0395414  validationAccuracy:  0.6758\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:  0.030796  validationAccuracy:  0.706\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:  0.0257612  validationAccuracy:  0.7146\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:  0.0117295  validationAccuracy:  0.7118\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:  0.0140102  validationAccuracy:  0.7232\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:  0.00748311  validationAccuracy:  0.7162\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:  0.0199255  validationAccuracy:  0.6924\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:  0.0171467  validationAccuracy:  0.7018\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:  0.0113621  validationAccuracy:  0.7172\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:  0.00627618  validationAccuracy:  0.7182\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:  0.00717831  validationAccuracy:  0.7262\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:  0.00771076  validationAccuracy:  0.709\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:  0.0128291  validationAccuracy:  0.7036\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:  0.0274562  validationAccuracy:  0.6996\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:  0.0127442  validationAccuracy:  0.718\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:  0.00946564  validationAccuracy:  0.7244\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:  0.00965333  validationAccuracy:  0.7118\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:  0.0242048  validationAccuracy:  0.6914\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:  0.0326532  validationAccuracy:  0.697\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:  0.0132046  validationAccuracy:  0.7148\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:  0.0147812  validationAccuracy:  0.703\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:  0.0055288  validationAccuracy:  0.7216\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:  0.00785965  validationAccuracy:  0.7054\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:  0.0167442  validationAccuracy:  0.7118\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:  0.0136041  validationAccuracy:  0.714\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:  0.0263138  validationAccuracy:  0.7\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:  0.0163439  validationAccuracy:  0.7122\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:  0.0101564  validationAccuracy:  0.7106\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:  0.0212567  validationAccuracy:  0.7044\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:  0.00587336  validationAccuracy:  0.727\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:  0.00660319  validationAccuracy:  0.724\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:  0.00842373  validationAccuracy:  0.7124\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:  0.0230631  validationAccuracy:  0.6972\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:  0.0374904  validationAccuracy:  0.6808\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:  0.0214895  validationAccuracy:  0.7126\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:  0.0112357  validationAccuracy:  0.7196\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:  0.00795301  validationAccuracy:  0.718\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:  0.013072  validationAccuracy:  0.7112\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:  0.0135753  validationAccuracy:  0.7044\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:  0.0145422  validationAccuracy:  0.6996\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:  0.0102767  validationAccuracy:  0.7148\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:  0.00713181  validationAccuracy:  0.7262\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:  0.00527997  validationAccuracy:  0.7218\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:  0.00663533  validationAccuracy:  0.7122\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:  0.00960766  validationAccuracy:  0.706\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:  0.0160677  validationAccuracy:  0.6974\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:  0.0375335  validationAccuracy:  0.697\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:  0.00593452  validationAccuracy:  0.7232\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:  0.0153907  validationAccuracy:  0.7122\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:  0.0265108  validationAccuracy:  0.7058\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:  0.0307055  validationAccuracy:  0.691\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:  0.0148943  validationAccuracy:  0.704\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:  0.0298183  validationAccuracy:  0.7042\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:  0.0142685  validationAccuracy:  0.7136\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:  0.0109832  validationAccuracy:  0.7116\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:  0.0218269  validationAccuracy:  0.7032\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:  0.013551  validationAccuracy:  0.6996\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:  0.0151015  validationAccuracy:  0.7148\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:  0.030661  validationAccuracy:  0.705\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:  0.00807477  validationAccuracy:  0.7202\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:  0.00548483  validationAccuracy:  0.7202\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:  0.00908303  validationAccuracy:  0.7134\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:  0.00752523  validationAccuracy:  0.7222\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:  0.00431482  validationAccuracy:  0.7208\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:  0.00866645  validationAccuracy:  0.7186\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:  0.0119678  validationAccuracy:  0.7084\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:  0.00872158  validationAccuracy:  0.711\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:  0.0162735  validationAccuracy:  0.714\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:  0.00524249  validationAccuracy:  0.7146\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:  0.00662179  validationAccuracy:  0.727\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:  0.0101515  validationAccuracy:  0.7106\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:  0.0194728  validationAccuracy:  0.7068\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:  0.0186055  validationAccuracy:  0.7094\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:  0.00949824  validationAccuracy:  0.7194\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:  0.0066678  validationAccuracy:  0.7072\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:  0.0104065  validationAccuracy:  0.721\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:  0.0108913  validationAccuracy:  0.7162\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:  0.00800066  validationAccuracy:  0.7264\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:  0.00875579  validationAccuracy:  0.7112\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:  0.0067338  validationAccuracy:  0.7256\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:  0.00535973  validationAccuracy:  0.7218\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:  0.00227523  validationAccuracy:  0.729\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:  0.00272112  validationAccuracy:  0.7312\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:  0.00317657  validationAccuracy:  0.7244\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:  0.00630136  validationAccuracy:  0.7204\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:  0.00381432  validationAccuracy:  0.7222\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:  0.00805849  validationAccuracy:  0.7114\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:  0.00350524  validationAccuracy:  0.7178\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:  0.00346403  validationAccuracy:  0.7226\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:  0.00231386  validationAccuracy:  0.7308\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:  0.00427727  validationAccuracy:  0.7256\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:  0.00388729  validationAccuracy:  0.719\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:  0.00291167  validationAccuracy:  0.724\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:  0.00167131  validationAccuracy:  0.7252\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:  0.000907378  validationAccuracy:  0.736\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:  0.00204286  validationAccuracy:  0.7324\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:  0.0034559  validationAccuracy:  0.7306\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:  0.00156813  validationAccuracy:  0.7274\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:  0.00214927  validationAccuracy:  0.7266\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:  0.00171039  validationAccuracy:  0.7216\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:  0.00171328  validationAccuracy:  0.7256\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:  0.000569648  validationAccuracy:  0.733\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:  0.000974117  validationAccuracy:  0.7344\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:  0.000771635  validationAccuracy:  0.7354\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:  0.000968645  validationAccuracy:  0.7312\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:  0.00204063  validationAccuracy:  0.73\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:  0.00124887  validationAccuracy:  0.7334\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:  0.000647838  validationAccuracy:  0.7344\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:  0.000726005  validationAccuracy:  0.7348\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:  0.000910328  validationAccuracy:  0.7366\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:  0.000519803  validationAccuracy:  0.7342\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:  0.00169459  validationAccuracy:  0.7384\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:  0.000725473  validationAccuracy:  0.74\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:  0.001775  validationAccuracy:  0.7304\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:  0.00158674  validationAccuracy:  0.732\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:  0.000718502  validationAccuracy:  0.7354\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:  0.000257221  validationAccuracy:  0.7426\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:  0.000605419  validationAccuracy:  0.741\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:  0.000579783  validationAccuracy:  0.7364\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:  0.000217366  validationAccuracy:  0.7414\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:  0.000399185  validationAccuracy:  0.7458\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:  0.00180939  validationAccuracy:  0.7352\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:  0.000307861  validationAccuracy:  0.741\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:  0.00076422  validationAccuracy:  0.7362\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:  0.000434506  validationAccuracy:  0.7352\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:  0.00117104  validationAccuracy:  0.7348\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:  0.000557413  validationAccuracy:  0.738\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:  0.00040189  validationAccuracy:  0.7348\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:  0.000765645  validationAccuracy:  0.7376\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:  0.000832918  validationAccuracy:  0.739\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:  0.000963548  validationAccuracy:  0.7404\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:  0.00145979  validationAccuracy:  0.7326\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:  0.000774123  validationAccuracy:  0.7396\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:  0.0042403  validationAccuracy:  0.7276\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:  0.000871369  validationAccuracy:  0.7366\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:  0.000422789  validationAccuracy:  0.7394\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:  0.000170745  validationAccuracy:  0.7464\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:  0.000244462  validationAccuracy:  0.7358\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:  0.000205938  validationAccuracy:  0.743\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:  0.000179971  validationAccuracy:  0.7414\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:  0.000336639  validationAccuracy:  0.7424\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:  7.4776e-05  validationAccuracy:  0.748\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:  0.00010144  validationAccuracy:  0.743\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:  0.000181697  validationAccuracy:  0.7412\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:  0.000857158  validationAccuracy:  0.7396\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:  8.97492e-05  validationAccuracy:  0.7432\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:  5.28884e-05  validationAccuracy:  0.7442\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:  0.000169152  validationAccuracy:  0.748\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:  0.000105961  validationAccuracy:  0.7492\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:  5.50477e-05  validationAccuracy:  0.748\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:  6.01187e-05  validationAccuracy:  0.7484\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:  2.93979e-05  validationAccuracy:  0.7468\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:  7.5348e-05  validationAccuracy:  0.7476\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:  8.21802e-05  validationAccuracy:  0.7472\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:  2.80046e-05  validationAccuracy:  0.7484\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:  2.36581e-05  validationAccuracy:  0.7498\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:  2.24599e-05  validationAccuracy:  0.7494\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:  3.20478e-05  validationAccuracy:  0.7492\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:  3.20589e-05  validationAccuracy:  0.7492\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:  2.5358e-05  validationAccuracy:  0.7502\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:  2.21967e-05  validationAccuracy:  0.7502\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:  1.55478e-05  validationAccuracy:  0.7486\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:  2.36839e-05  validationAccuracy:  0.7468\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:  2.9347e-05  validationAccuracy:  0.7492\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:  2.31955e-05  validationAccuracy:  0.7486\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:  2.12075e-05  validationAccuracy:  0.7496\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:  1.18043e-05  validationAccuracy:  0.7494\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:  1.72884e-05  validationAccuracy:  0.7494\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:  2.14716e-05  validationAccuracy:  0.75\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:  1.898e-05  validationAccuracy:  0.75\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:  1.83078e-05  validationAccuracy:  0.7476\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:  1.02606e-05  validationAccuracy:  0.749\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:  1.53701e-05  validationAccuracy:  0.7516\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:  2.36929e-05  validationAccuracy:  0.7526\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:  1.46878e-05  validationAccuracy:  0.7508\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:  1.67445e-05  validationAccuracy:  0.7492\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:  1.14377e-05  validationAccuracy:  0.7478\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:  1.24853e-05  validationAccuracy:  0.7494\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:  2.78e-05  validationAccuracy:  0.7488\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:  1.34969e-05  validationAccuracy:  0.7506\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:  1.39812e-05  validationAccuracy:  0.753\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:  9.70141e-06  validationAccuracy:  0.7504\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7436342000961303\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc49fc84f60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
