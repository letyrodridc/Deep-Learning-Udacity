{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 16:\n",
      "Image - Min Value: 2 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAGrxJREFUeJzt3cmPZYd1H+Bz31RjV1f1TDZFqkmKkkxbQhREjKIgFmAj\ngIEMmyRw/rF4nWyyCCIjQOAsbCNAlNhJJBi2pIgzKYns5tRzV3VXvffqvSyyUYBszlGzqBx83/7U\nue9Ov7qr37BerwMA6Gn0RR8AAPD5EfQA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT\n9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGpt80Qfwebl95976rHaNRrX/lypz1V3D\nMNTmCvvW4zM79fXfVdw3Xucnq7vOUuFnnbnVapWfOa3tWq/z93BlJqL2u6qqx1gbO8tdEet14f4o\nnvv5Mn9jLVe1H/bl61d/7afTFz0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0\nJugBoDFBDwCNCXoAaEzQA0Bjbdvrlstlaa7ShlZvhDq7lrdq610U2p3WxdaqSkXTutheV7UuHOXZ\nXeVfY9cZnsazfF7WxVq+yq5qE9pZvge67qruq16zVaGJbnX6xVVE+qIHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI21LbWpqpTajMfjz+FI/t8qxxcRMR7V\njnEYVfbViiIqv63eaVMbLBXvnGG5R/V0VLpfqr+reg+Xfl1xVe1ePLsCnaqzLRQ627Kvynu4WmpT\n8sV12viiB4DOBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaKxte91oVPsfpt6slVc5xtG4+LtKLXQRo8LcUPz/cXSGjWH1WrP8SLUh6yxbzUrFcGfc1lZq\nDize95VjPOvzcZaNchVnev8WVZtHx5VnevzFnQ9f9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQ\nA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgsbalNjHUygoq9SOjYkHKUPg3azQUixGGWrHKUDjI8bpY\noFM5IUWnxaKZoVAqNJvNSrsWi0V65vT0tLQrCrdVtcSlXP5SGioW6BQOcV07whiK74/1GRZwVYqZ\nzrrkp6Jafjab5vPldF175zwNvugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGg\nMUEPAI0JegBoTNADQGOCHgAaa9tetyy2k0WhSWooNsoNlWOs7io3QuXnhqg1B1bOx3hSu4VH49ox\nPjx8lJ65detWadelS5fSM+fOnSvtmhTOY7VlbFVs2KvtO7smtHKXXPVzq9ASWW1rO0vVBsbK/VG9\nh9er/NxyMS/tehp+8686AFAm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAx\nQQ8AjQl6AGhM0ANAY23b62JSaydbLBbpmUqTUUTEapmfGRdL+aqlVZWCvdG0du5ns1l65tGjh6Vd\nr7/xRmnuBz/4QXrmnXfeKe165pln0jMvv/xyadcrr7ySnrlx40Zp18HBQWmu0rw2n9cawyoNatUm\ntKi2ABba68ptbWfYDFdV2bcqtpxW7qsf/uiHpV3P/dPrpblf5YseABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADTWttTm9TdrpSXXr+cLBDYKZSwREaN8b0aM\nR7XCmHFtLGKdP8iPbt0srXrzjdfTM2+99VZp1/37D0pz5/f20jPf/OY3S7sqBUvvvfdeadfPfvaz\n9Mz29nZp1/PPP1+ae/HFF9Mzlec5ImJ/fz89M51OS7uGIV9OExGxOs0XslTKeiLOttSmWjRTmTvL\nkp+tra3SrqfBFz0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0BjQ7W95zfdH/yzf1H6Yf/yD/8wPfPd7363sirWhSapUbG9rtqs9XahHe4//cf/UNo1\nRP6SvfDCC6Vdz3/pS6W5nc18A1W5MawwU23+Ojo6Ss989tlnpV2ffvppaW4+n6dndnd3S7suXryY\nnnn55ZdLu27cyLfyRURcuHipNFexKJz75Rk25VXnyg17hVbP9br2bH7p+nO1esNf4YseABoT9ADQ\nmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQ2+aIP4PPy0Yc3S3P/\n/vvfT89Md86Vdn39a7+VntmYlVbFeqiVN+zu50tBXv2tr5Z2Pf/SS+mZvf0LpV3LZe18VGo6Norn\nfjLOX+yHtz8u7RqN8se4s/t8addsVnvtLE5O0jNHDx+Vdh0ePUjP/Pmf/klp18GlWjnNl298JT1z\n7dqzpV0XC8e4u1MrFIqh9v25LJXG1PpiRqv83FDc9TT4ogeAxgQ9ADQm6AGgMUEPAI0JegBoTNAD\nQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGisbXvdN154pjR39+Hd9My//df/prTru6/9\nbnrmn/yjPyjtmo1rl/pgtpGeee5irVFuZ2szPfPo6Li06/GTStNVxGKWnzs/rbVWbW1tpWfu3L5d\n2rW5Nc4PjQozEbF7Lv+7IiIezfPXej1flHaNCudjc6jdU6vF49LczZsfpGfefOut0q5p4T1w5fLV\n0q4bhRbLiIjLzz2XnhmK37qT0/y1Hq9rLZZPgy96AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxtq2171yodYk9Whrlp5591a+RSoi4s/+5N+lZ179\n2vOlXd/5zmuluSeP8/8Lzte1/x8fPjhMz2zuHpR2Tce1+2NnlG9Dq3W1Rfzi5++nZx7cu1/adWW8\nl56ZTEur4vz+fmnu6P6j9Mxokn+eIyI++yTfYnkw7JR2TVb5ZriIiIMLl9Izw1C7PzY38ufxw5+/\nW9q1OHlSmts9OJ+e2djYLe2KyDdSDqMv7rvaFz0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQ\nmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxtqc149aA0d7CVryD5+rO1ko43bz9Mz3z0xo9Lu+4990xp\n7u2bN9Mz//Ptt0q7lqf5opmh2KxycjIvzZ1f5Yt3Lp/Pl21ERGxdfik9s71TK1aZL/LnY13aFDEa\nj2uDq/z9sV0oY4mImD3J31db0+3SrtWodoyjoTC3ql218zv53/ag+Bn54ftvl+Y++PTj9Mzu+cul\nXZcvXk3P7GzX6q1eeuHZ0tyv8kUPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQWNv2uk/m+aariIjNrdP0zHrIz0REvLiXbzOav/Nuadd/u/390txf\nvP9+euanD++Wdq0LzVrrQqNZRMRqUmtQmx0/Sc9cOb9b2vV3vpdvoruyV2tCG9ZDemY8rr0+VsVr\nNhnlj/H2x7dKu2JSOB8750qrnkTt/bE8zT8vQ+R/V0TE0YN8G+gv36210D2e55+xiIgP7uebJdfj\nWtvjxmb+Wo+mtXfOP//H/7A093/t/rX/AgDwG0vQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0FjbUpv//FdvlOb2z+eLZnanG6Vd04182clbn31S2nWyrJWd7Lzw\ncnrm2uVrpV23f3kzPbNaLEq7luNakcjxKl8KcnhS2/VHf/Sv0jPf+863Srt+//d+Nz2zXNbO/Whe\nGovpkC9x2Z/mZyIiHi9O0jPT7dozdnr0uDZXOP+rRbFAZ7lMz1zd2yvt+uCTo9LcsM5f62XU7o9H\nT/LXbF3r6nkqfNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQ\nmKAHgMYEPQA01ra9brSaluZOHubbna4+f7606+pLfzs9c+fjN0u7jm7XGqGe+dJz6ZnxZr6VLyJi\nNh2nZ9ZPalVos0nt1j8dr9Izo+K/06+/nb/Wdx48LO06eZJvazuY1H7Y5pBvAIyImBeaCsfLWmXY\n+fMH6Zn1tNZeF/MHpbFhnb9mkb99IyJiNM4/m+PHtWdze7pdmhsVahFPo9bmN53lG0s3ivf90+CL\nHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA01rbU5sLO\nZmlu/9xOeubahUulXXtb+fKG9V6+bCMi4vRkqzT36c3b6ZlHi5ulXcuTfCnF8rBWWrJRLLW5cGU/\nPbO/X7tmr7327fTMs88+U9q1Ps6fx71iedF6lC8EiYi4e5IvBfnsYe3+mO5fTc9sTopFWsWimaNH\n+aKq1bpW4rIoFE59eP9eadeFZ6+X5r587lx65uZn90u7Ll24kp45N1NqAwB8DgQ9ADQm6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGisbXvdue11aW5nO98wNJ3W\nWok2I9/WtrtZa/66+NWXSnOz/Xwb2tHyuLRrOso3ZK2XtTau+UntGLf38i2A01mt1ezq1XyD2vmD\nvdKujz/6ZXrm3LL2+vjhm39Tmnu0kz/3v/Pyb5d2vX8n37x2uK61tV25crk0tz7JN/O98Py10q6N\ng3zT5qP7XyvtevGl2rvqTqEt7+RHPy3tWpws0zN/8ZPXS7ueBl/0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjfVtr9uv/bTRRr717slpvkUqImL4\n9P30zGpca2v76NOT0tzth2+mZ4bZrLRrZ2cnPTMqNN5FRMxmtfvj3MluemZ7O9+6FhExn+fbDW/d\nulXa9dc/+lF65r9MNku77h7X2h6Hnf30zMbpV0u7fvrmO+mZ0VB7Ng/O1e7hr9/Itxv+9iv5mYiI\njd38N+Hf/c63ars2am2Pu7ur9Mwbb9XO/fsfP0zPnET++J4WX/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG2pTZPVkeluY1R/n+fw0Vt1+j0MD0z3akV\npNy5my9hiIj4r3/54/zQeKO06/z5g/TMg8Pa7xqGWrHK937vH6RnXnvt26Vd7773bnrm+PC4tGty\nkn8VfHj3TmnXo6Na+ctzV/L3/l/+j78p7TopFAodH90r7bo5zr8HIiJevJ4vj7rzyXulXdd3XkzP\nbIxq5Vbrea0kbLTOn8eNc7WimVs/fis9c/3i+dKup8EXPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4A\nGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGNt2+vuPao1Ql3e2k3PzCa1/5fWQ35ucVrb\ndfHyhdLcCzfyrVVHxyelXbPZND1TLMiKUaGlMCLi6uVL6Zkh1qVdv/Pqq+mZd99+p7TrwaW99Myd\nx7XmwMNH90tzdzc+Tc+MJ7UbZLnMt9c9eFBrr/tb37hRmvvys/m2x4/v5s9hRMTlL72SnhmG/DmM\niHj84HZpbrnKv3e2LhTf3duFe3hWa8x8GnzRA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0J\negBoTNADQGOCHgAaE/QA0JigB4DG+pbafFYrEpkOp/mZS7XijKNCscqDB8elXReuPFea+87f/0p6\n5mRRKxSaz/O/bTrNF+FERBwf185jFApq7t+rlZ0cHebP4x//8fdLuy4eXEzPzOe10pLxZFyam68W\n6ZnRcX4mImI2yx/jMNTeORuFMqeIiNXpMj2zOF2Vdo0q7VGj/Lv0/6idxyeH+VKbg618MVBExN7W\nVnpmMq6ej1+fL3oAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DG2rbXfTLPNxlFRNz77GZ+JmoNWfvn9tIzp+uhtOvw05+X5m48nz/Gq5culXbdvv1Z\neuaVV75a2nXrVv46R0R8/PGt9Mzli/lmuIiImx98mJ7Z2tgs7bp3t9CwN9Ra16rmhWd6WBXb2sYb\n6ZnxuPhsPj4qzW3snkvPbJ8WWz0Ln4THp7V2w8m01m64Ocnf+/vFe/iV576Rnpk/rt0fT4MvegBo\nTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMbattft\nfSXf7BQRMR7n266O1k9Ku1any/TMrHjJju7eLs29/pP/lZ5Zz2sNakPh/84//9M/K+1arU5Lc0Oh\ngOqvfvij0q6Dg4P0zP7e+dKuj44+zg+Nai1jx8e1ZsnJNH9fTYZaW9tqnX8211G7p+aF90BExGxr\nJz1zdWe3tOv0JH/NFuvadd7e2i7NjVf5+/GDj2otlvce55s2lye1Nr+nwRc9ADQm6AGgMUEPAI0J\negBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGisbanNRqHgICJivcz/77M6nZV2\nHa/yu+bLWgHGeDktzQ3zfGHPJx99Utr1zPXr6Zmd7VoBxnqVLy+KiDh6fJifOTqq7TrM77p67Vpp\n12KxSM/snqud+729vdLculCis5gfl3aNl/n2ov3ztcKY+aJW/vLWe++mZy5+7dXSrtPC/XF08ri0\n6/5RrSRsNM4XGP3k539d2vXWrXzZ1850o7TrafBFDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9\nADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0Fjb9rpro5dKc+NJ/pRMJpulXZNZvs1oVji+iIjx\nujZ3b+9eemZr8+3SrmvP5JvXZtNac+DjJ7VmrY2NfKvZycm8tOvw8FF+KH94ERFx/fK59My3X71R\n2vXO8cXS3C+O8s2Nj2/fKe2ajfNtbS9dPi3tGs1q9+KP3/vv6ZmjR/dLu3aH/Pt0sSrcvxHx6LD2\nvMwO8tfsndv5FrqIiI2d/IO2qJXyPRW+6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY21Lbb716t8rzQ2j/P8+s9m0tGs6yZfaDMXWkgcPH5Tmdrf30jMX\nr1wo7ZrPT9Iz6/WqtOvCer80NyrcH8tlvowlIuL0ND93fFIrVtka8o0bi1H+ekVEPJnl76mIiKHw\n05ajo9KuzYtb6Zlh+xelXbPd2nl8OH6Ynnn/7q3Sro2j/PNy9PDj0q7JZr5gKSJissxf65PTWqHQ\naCi88wvvjqfFFz0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0BjbdvrDi4clObG43F6ptJoFhExHuVP/zDUdp0UmuEiIqbTfEvT9rBT2rW1lW8Mm81m\npV2V6xwRcXKSP49PnuSb4SIiFstFeuZ0cVjaNZnkz+Ot2CztulecWz68k56ZP8o3vEVErK/l7/v5\nbq0J7cmkds1Ox/l2w81xrf1yEvln8+Gntd+1M6294/a2ttMzQ+32iDjNVymO1sVdT4EvegBoTNAD\nQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMbattdNJrWf\n9vhxvoGqumt3ZyM9Mwy19qlKC11ExFahEWqyqDXDna5W6Zlqe916XauSWq/zjXKzaa2tbTTKX7NV\nrTgwJuN5euazqJ371bj2vEyGo/TMOPK/KyJiPeSv83J8XNq1GPItdBERq2X+eVmN8zMRESfL/Htx\nHrU2v61xvikvImKxzJ/HcfFbd4j8e3g0qr0XnwZf9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQ\nA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgsbalNvfu3SvNVUpjdnZqTSKjUX7XYlErwKiWvxwcHKRn\nqse4XOaLRKJQLhERsVqdlubWq/y+ZaFsIyJiPj9Jz5xMKucwIib537WxyBceRUQsnzwqzW1sHaZn\nxqPa+ZiM8q/G0ar23TRa1UqPhiF/Dw+rWpnTbJK/h7e2as/YdLP2TFdKsYba6YhR5AfHxUKyp8EX\nPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGNt\n2+tGw7g0t3d+Lz0zm9Xapyotbycn+UaziIjT01qTVMVkUjv343F+blVorIqIWC5rTVIbG7UWwIrK\nNau2FA7TaXpmcVj7TlgefVSaO3d+np6ZbuR/V0TE/iw/N13Udo2H4j1caG5cjWrvgePHD9Mzi9Fx\naddQPB/jdf79cVpsv5yM8vf+/KR2Pp4GX/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4Ie\nABoT9ADQmKAHgMYEPQA0JugBoLG2pTbb2zuluaHyv8+6tOpMDUOtvKFSGnOWBTqjQrlERMR0Wrv1\nK6exUtYTETEtFM3MprVdo838rl8c5kuZIiL2Rw9Kczs7+Wv9YHOrtGt3lL8/Zqt8IVZExDpqRVXr\nOCrNVRwe5kttTie1F+N6XXt/VPp61sXys3mhkOzx0WFp19Pgix4AGhP0ANCYoAeAxgQ9ADQm6AGg\nMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaCxYb3+/6B6DQAo8UUPAI0JegBoTNAD\nQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA\n0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGhP0ANCYoAeAxv43zXxmhIsKBtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5064f44550>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 16\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    maxX = 255\n",
    "    return (x/maxX)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    numLabels = 10\n",
    "    res = np.zeros( (len(x), numLabels) )\n",
    "    res[np.arange(len(x)),x] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(name=\"x\", shape=(None,image_shape[0],image_shape[1],image_shape[2]), dtype=tf.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(name=\"y\",shape=[None,n_classes], dtype=tf.float32)\n",
    "    return y\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(name=\"keep_prob\",shape=None, dtype=tf.float32)\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    F_W = tf.Variable(tf.truncated_normal( [conv_ksize[0], conv_ksize[1], int(x_tensor.shape[3]), conv_num_outputs], stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(conv_num_outputs, dtype=tf.float32))\n",
    "    padding = \"SAME\"\n",
    "    conv2d = tf.nn.conv2d(input=x_tensor, filter=F_W, strides=[1,*conv_strides,1], padding=padding) + F_b\n",
    "    conv2d  = tf.nn.bias_add(conv2d , F_b)\n",
    "   # conv2d =  tf.nn.relu(conv2d)\n",
    "    pooling = tf.nn.max_pool(conv2d, ksize=[1,pool_ksize[0],pool_ksize[1],1], padding=padding, strides=[1,*pool_strides,1])\n",
    "    return pooling \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    new_dim = int(np.prod(x_tensor.shape[1:]))\n",
    "    new_tensor = tf.reshape(tensor=x_tensor, shape=[tf.shape(x_tensor)[0],new_dim])\n",
    "    return new_tensor\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    F_W = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], stddev=0.01))\n",
    "    F_b = tf.Variable(tf.zeros(num_outputs, dtype=tf.float32))\n",
    "\n",
    "    output = tf.add(tf.matmul(x_tensor,F_W),F_b)\n",
    "\n",
    "    return output\n",
    "    # tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 4, 4, 256)\n",
      "(?, 4096)\n",
      "(?, 512)\n",
      "(?, 32, 32, 3)\n",
      "(?, 4, 4, 256)\n",
      "(?, 4096)\n",
      "(?, 512)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv_num_outputs = 10\n",
    "    print(x.shape)\n",
    "    #x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 64, (1,1), (1,1), (2,2), (2,2))\n",
    "   # x = tf.layers.dropout(x, rate=1-keep_prob)\n",
    "    x = conv2d_maxpool(x, 128, (1,1), (1,1), (4,4), (2,2))\n",
    "   # x = tf.layers.dropout(x, rate=1-keep_prob)\n",
    "    x = conv2d_maxpool(x, 256, (1,1), (1,1), (4,4), (2,2))\n",
    "    \n",
    "    \n",
    "    #tf.layers.dropout(inputs, rate=1-keep_prob[1])\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    print(x.shape)\n",
    "    x = flatten(x)\n",
    "    print(x.shape)\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    x = fully_conn(x, 2048)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 1024)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "    \n",
    "    print(x.shape)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, \n",
    "                                        y:label_batch,\n",
    "                                        keep_prob:1.0}) \n",
    "    acc = session.run(accuracy, \n",
    "                feed_dict={x:valid_features, \n",
    "                           y:valid_labels, \n",
    "                           keep_prob:1.0})\n",
    "    print(\"loss: \",loss,\" validationAccuracy: \",acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 5000\n",
    "batch_size = 2048\n",
    "keep_probability = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  2.8547  validationAccuracy:  0.1134\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  2.20239  validationAccuracy:  0.1734\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  2.08687  validationAccuracy:  0.2318\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  2.00374  validationAccuracy:  0.2804\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  1.90392  validationAccuracy:  0.3088\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  1.86822  validationAccuracy:  0.3276\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  1.85456  validationAccuracy:  0.3318\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  1.79981  validationAccuracy:  0.351\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  1.72989  validationAccuracy:  0.3784\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  1.70272  validationAccuracy:  0.3808\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  1.65109  validationAccuracy:  0.3976\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  1.61153  validationAccuracy:  0.4094\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  1.58265  validationAccuracy:  0.4136\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  1.61672  validationAccuracy:  0.3944\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  1.49987  validationAccuracy:  0.4258\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  1.52533  validationAccuracy:  0.4146\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  1.45434  validationAccuracy:  0.4446\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  1.41452  validationAccuracy:  0.452\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:  1.39374  validationAccuracy:  0.4606\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:  1.38361  validationAccuracy:  0.4544\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:  1.38296  validationAccuracy:  0.4612\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:  1.31655  validationAccuracy:  0.4796\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:  1.35298  validationAccuracy:  0.4664\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:  1.33753  validationAccuracy:  0.4646\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:  1.28081  validationAccuracy:  0.49\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:  1.29137  validationAccuracy:  0.4516\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:  1.29337  validationAccuracy:  0.4572\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:  1.25303  validationAccuracy:  0.4776\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:  1.24123  validationAccuracy:  0.4882\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:  1.1835  validationAccuracy:  0.4936\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:  1.14397  validationAccuracy:  0.4892\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:  1.17097  validationAccuracy:  0.483\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:  1.15065  validationAccuracy:  0.4766\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:  1.07968  validationAccuracy:  0.495\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:  1.06017  validationAccuracy:  0.509\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:  1.02347  validationAccuracy:  0.5132\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:  0.990364  validationAccuracy:  0.5116\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:  1.00232  validationAccuracy:  0.5\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:  1.29884  validationAccuracy:  0.4038\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:  1.58077  validationAccuracy:  0.3698\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:  1.36059  validationAccuracy:  0.4232\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:  1.36438  validationAccuracy:  0.4276\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:  1.3072  validationAccuracy:  0.4338\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:  1.19252  validationAccuracy:  0.4808\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:  1.12805  validationAccuracy:  0.4872\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:  1.06702  validationAccuracy:  0.5046\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:  1.0157  validationAccuracy:  0.5074\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:  0.984818  validationAccuracy:  0.5122\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:  0.947365  validationAccuracy:  0.5116\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:  0.900414  validationAccuracy:  0.511\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:  0.890447  validationAccuracy:  0.4976\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:  0.855525  validationAccuracy:  0.4956\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:  0.786605  validationAccuracy:  0.5178\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:  0.763408  validationAccuracy:  0.5224\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:  0.803236  validationAccuracy:  0.5068\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:  0.745252  validationAccuracy:  0.5212\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:  0.720305  validationAccuracy:  0.526\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:  0.711814  validationAccuracy:  0.5166\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:  0.687517  validationAccuracy:  0.5246\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:  0.692792  validationAccuracy:  0.5172\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:  0.741017  validationAccuracy:  0.5052\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:  0.690978  validationAccuracy:  0.506\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:  0.81908  validationAccuracy:  0.4734\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:  0.765082  validationAccuracy:  0.4954\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:  0.680266  validationAccuracy:  0.5148\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:  0.656151  validationAccuracy:  0.5172\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:  0.643995  validationAccuracy:  0.5214\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:  0.578927  validationAccuracy:  0.5234\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:  0.531728  validationAccuracy:  0.5192\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:  0.504348  validationAccuracy:  0.5294\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:  0.503571  validationAccuracy:  0.5236\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:  0.562712  validationAccuracy:  0.5044\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:  0.515406  validationAccuracy:  0.5038\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:  0.534874  validationAccuracy:  0.5038\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:  0.573996  validationAccuracy:  0.5042\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:  0.505295  validationAccuracy:  0.5008\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:  0.476384  validationAccuracy:  0.5114\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:  0.421646  validationAccuracy:  0.5274\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:  0.403604  validationAccuracy:  0.5174\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:  0.360926  validationAccuracy:  0.5194\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:  0.342588  validationAccuracy:  0.5134\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:  0.333752  validationAccuracy:  0.5184\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:  0.322321  validationAccuracy:  0.511\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:  0.950908  validationAccuracy:  0.4056\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:  5.79041  validationAccuracy:  0.2454\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:  39.1495  validationAccuracy:  0.1532\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:  7.82646  validationAccuracy:  0.1402\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:  16.1545  validationAccuracy:  0.1272\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:  9.89271  validationAccuracy:  0.1438\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:  4.89924  validationAccuracy:  0.2006\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:  3.63344  validationAccuracy:  0.1766\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:  2.43035  validationAccuracy:  0.2082\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:  2.07108  validationAccuracy:  0.2876\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:  1.92818  validationAccuracy:  0.3366\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:  1.72395  validationAccuracy:  0.3618\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:  1.67716  validationAccuracy:  0.3576\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:  1.63237  validationAccuracy:  0.3774\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:  1.58953  validationAccuracy:  0.401\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:  1.56924  validationAccuracy:  0.4116\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:  1.53875  validationAccuracy:  0.4238\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:  1.51459  validationAccuracy:  0.4278\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:  1.49576  validationAccuracy:  0.4322\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:  1.47985  validationAccuracy:  0.44\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:  1.45822  validationAccuracy:  0.4434\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:  1.44154  validationAccuracy:  0.446\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:  1.42755  validationAccuracy:  0.4474\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:  1.4138  validationAccuracy:  0.4524\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:  1.39889  validationAccuracy:  0.455\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:  1.38416  validationAccuracy:  0.4556\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:  1.37072  validationAccuracy:  0.4564\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:  1.35839  validationAccuracy:  0.4604\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:  1.34578  validationAccuracy:  0.4606\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:  1.33322  validationAccuracy:  0.4636\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:  1.32108  validationAccuracy:  0.466\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:  1.3092  validationAccuracy:  0.464\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:  1.29724  validationAccuracy:  0.466\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:  1.28566  validationAccuracy:  0.467\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:  1.27469  validationAccuracy:  0.4678\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:  1.26399  validationAccuracy:  0.471\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:  1.25257  validationAccuracy:  0.4722\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:  1.24153  validationAccuracy:  0.4754\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:  1.23066  validationAccuracy:  0.476\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:  1.22007  validationAccuracy:  0.4802\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:  1.21027  validationAccuracy:  0.4818\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:  1.19912  validationAccuracy:  0.4816\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:  1.18697  validationAccuracy:  0.4844\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:  1.17664  validationAccuracy:  0.486\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:  1.16527  validationAccuracy:  0.489\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:  1.15322  validationAccuracy:  0.492\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:  1.14192  validationAccuracy:  0.4932\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:  1.13055  validationAccuracy:  0.4954\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:  1.11896  validationAccuracy:  0.497\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:  1.10756  validationAccuracy:  0.5002\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:  1.09743  validationAccuracy:  0.5036\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:  1.08678  validationAccuracy:  0.5038\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:  1.07537  validationAccuracy:  0.5072\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:  1.06524  validationAccuracy:  0.5074\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:  1.05564  validationAccuracy:  0.507\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:  1.04515  validationAccuracy:  0.509\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:  1.03514  validationAccuracy:  0.5136\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:  1.02561  validationAccuracy:  0.5138\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:  1.0153  validationAccuracy:  0.5152\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:  1.00526  validationAccuracy:  0.515\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:  0.99341  validationAccuracy:  0.5154\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:  0.982868  validationAccuracy:  0.5152\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:  0.97427  validationAccuracy:  0.5162\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:  0.969156  validationAccuracy:  0.508\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:  0.956966  validationAccuracy:  0.5112\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:  0.941322  validationAccuracy:  0.5166\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:  0.929581  validationAccuracy:  0.5216\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:  0.920178  validationAccuracy:  0.5186\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:  0.909387  validationAccuracy:  0.5186\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:  0.899541  validationAccuracy:  0.5202\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:  0.889973  validationAccuracy:  0.5196\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:  0.884296  validationAccuracy:  0.5154\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:  0.883241  validationAccuracy:  0.5126\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:  0.867233  validationAccuracy:  0.5132\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:  0.850953  validationAccuracy:  0.5176\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:  0.839588  validationAccuracy:  0.5186\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:  0.830983  validationAccuracy:  0.517\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:  0.820982  validationAccuracy:  0.5176\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:  0.812181  validationAccuracy:  0.5172\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:  0.807565  validationAccuracy:  0.5188\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:  0.801803  validationAccuracy:  0.518\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:  0.787784  validationAccuracy:  0.52\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:  0.773458  validationAccuracy:  0.5242\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:  0.805678  validationAccuracy:  0.513\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:  0.943124  validationAccuracy:  0.4746\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:  0.957467  validationAccuracy:  0.4694\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:  0.782358  validationAccuracy:  0.5112\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:  0.795264  validationAccuracy:  0.503\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:  0.770972  validationAccuracy:  0.5082\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:  0.735941  validationAccuracy:  0.5176\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:  0.744052  validationAccuracy:  0.5102\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:  0.721308  validationAccuracy:  0.5178\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:  0.708347  validationAccuracy:  0.519\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:  0.69529  validationAccuracy:  0.52\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:  0.681568  validationAccuracy:  0.5202\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:  0.672213  validationAccuracy:  0.521\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:  0.660073  validationAccuracy:  0.5246\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:  0.655092  validationAccuracy:  0.5256\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:  0.647463  validationAccuracy:  0.5242\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:  0.64008  validationAccuracy:  0.5242\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:  0.638872  validationAccuracy:  0.5232\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:  0.631761  validationAccuracy:  0.5272\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:  0.617975  validationAccuracy:  0.529\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:  0.607896  validationAccuracy:  0.5254\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:  0.600982  validationAccuracy:  0.5272\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:  0.598758  validationAccuracy:  0.5276\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:  0.590214  validationAccuracy:  0.5276\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:  0.578891  validationAccuracy:  0.5272\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:  0.57221  validationAccuracy:  0.5304\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:  0.566789  validationAccuracy:  0.528\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:  0.559432  validationAccuracy:  0.5276\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:  0.549699  validationAccuracy:  0.5294\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:  0.541102  validationAccuracy:  0.5284\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:  0.529985  validationAccuracy:  0.533\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:  0.522905  validationAccuracy:  0.5294\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:  0.515604  validationAccuracy:  0.5326\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:  0.511253  validationAccuracy:  0.5286\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss:  0.529208  validationAccuracy:  0.52\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss:  0.499525  validationAccuracy:  0.531\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss:  0.512818  validationAccuracy:  0.5268\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss:  0.496201  validationAccuracy:  0.5272\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss:  0.477157  validationAccuracy:  0.5322\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss:  0.476391  validationAccuracy:  0.5308\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss:  0.480711  validationAccuracy:  0.5296\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss:  0.481005  validationAccuracy:  0.5272\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss:  0.482849  validationAccuracy:  0.524\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss:  0.489862  validationAccuracy:  0.5228\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss:  0.498768  validationAccuracy:  0.5186\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss:  0.481649  validationAccuracy:  0.521\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss:  0.445623  validationAccuracy:  0.5274\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss:  0.423202  validationAccuracy:  0.5292\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss:  0.441195  validationAccuracy:  0.5142\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss:  0.419451  validationAccuracy:  0.5192\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss:  0.41317  validationAccuracy:  0.527\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss:  0.407518  validationAccuracy:  0.5278\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss:  0.416841  validationAccuracy:  0.5248\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss:  0.430799  validationAccuracy:  0.5164\n",
      "Epoch 221, CIFAR-10 Batch 1:  loss:  0.394233  validationAccuracy:  0.5256\n",
      "Epoch 222, CIFAR-10 Batch 1:  loss:  0.393624  validationAccuracy:  0.5226\n",
      "Epoch 223, CIFAR-10 Batch 1:  loss:  0.387132  validationAccuracy:  0.52\n",
      "Epoch 224, CIFAR-10 Batch 1:  loss:  0.37149  validationAccuracy:  0.5236\n",
      "Epoch 225, CIFAR-10 Batch 1:  loss:  0.364511  validationAccuracy:  0.523\n",
      "Epoch 226, CIFAR-10 Batch 1:  loss:  0.366186  validationAccuracy:  0.5186\n",
      "Epoch 227, CIFAR-10 Batch 1:  loss:  0.356357  validationAccuracy:  0.5228\n",
      "Epoch 228, CIFAR-10 Batch 1:  loss:  0.347448  validationAccuracy:  0.5206\n",
      "Epoch 229, CIFAR-10 Batch 1:  loss:  0.345366  validationAccuracy:  0.5198\n",
      "Epoch 230, CIFAR-10 Batch 1:  loss:  0.38797  validationAccuracy:  0.507\n",
      "Epoch 231, CIFAR-10 Batch 1:  loss:  0.363999  validationAccuracy:  0.5236\n",
      "Epoch 232, CIFAR-10 Batch 1:  loss:  0.475589  validationAccuracy:  0.5032\n",
      "Epoch 233, CIFAR-10 Batch 1:  loss:  0.419515  validationAccuracy:  0.5098\n",
      "Epoch 234, CIFAR-10 Batch 1:  loss:  0.459263  validationAccuracy:  0.5138\n",
      "Epoch 235, CIFAR-10 Batch 1:  loss:  0.385933  validationAccuracy:  0.5032\n",
      "Epoch 236, CIFAR-10 Batch 1:  loss:  0.364227  validationAccuracy:  0.5148\n",
      "Epoch 237, CIFAR-10 Batch 1:  loss:  0.337048  validationAccuracy:  0.5202\n",
      "Epoch 238, CIFAR-10 Batch 1:  loss:  0.358547  validationAccuracy:  0.5162\n",
      "Epoch 239, CIFAR-10 Batch 1:  loss:  0.347519  validationAccuracy:  0.521\n",
      "Epoch 240, CIFAR-10 Batch 1:  loss:  0.321036  validationAccuracy:  0.523\n",
      "Epoch 241, CIFAR-10 Batch 1:  loss:  0.321087  validationAccuracy:  0.525\n",
      "Epoch 242, CIFAR-10 Batch 1:  loss:  0.311605  validationAccuracy:  0.5228\n",
      "Epoch 243, CIFAR-10 Batch 1:  loss:  0.322705  validationAccuracy:  0.5174\n",
      "Epoch 244, CIFAR-10 Batch 1:  loss:  0.337716  validationAccuracy:  0.5072\n",
      "Epoch 245, CIFAR-10 Batch 1:  loss:  0.348157  validationAccuracy:  0.5056\n",
      "Epoch 246, CIFAR-10 Batch 1:  loss:  0.35636  validationAccuracy:  0.5088\n",
      "Epoch 247, CIFAR-10 Batch 1:  loss:  0.309376  validationAccuracy:  0.5198\n",
      "Epoch 248, CIFAR-10 Batch 1:  loss:  0.268975  validationAccuracy:  0.5266\n",
      "Epoch 249, CIFAR-10 Batch 1:  loss:  0.296145  validationAccuracy:  0.5088\n",
      "Epoch 250, CIFAR-10 Batch 1:  loss:  0.29014  validationAccuracy:  0.5034\n",
      "Epoch 251, CIFAR-10 Batch 1:  loss:  0.255166  validationAccuracy:  0.5172\n",
      "Epoch 252, CIFAR-10 Batch 1:  loss:  0.253536  validationAccuracy:  0.5172\n",
      "Epoch 253, CIFAR-10 Batch 1:  loss:  0.244163  validationAccuracy:  0.5166\n",
      "Epoch 254, CIFAR-10 Batch 1:  loss:  0.239583  validationAccuracy:  0.517\n",
      "Epoch 255, CIFAR-10 Batch 1:  loss:  0.235706  validationAccuracy:  0.5182\n",
      "Epoch 256, CIFAR-10 Batch 1:  loss:  0.225972  validationAccuracy:  0.518\n",
      "Epoch 257, CIFAR-10 Batch 1:  loss:  0.231328  validationAccuracy:  0.5122\n",
      "Epoch 258, CIFAR-10 Batch 1:  loss:  0.216958  validationAccuracy:  0.5198\n",
      "Epoch 259, CIFAR-10 Batch 1:  loss:  0.218334  validationAccuracy:  0.5136\n",
      "Epoch 260, CIFAR-10 Batch 1:  loss:  0.214152  validationAccuracy:  0.5156\n",
      "Epoch 261, CIFAR-10 Batch 1:  loss:  0.212927  validationAccuracy:  0.5172\n",
      "Epoch 262, CIFAR-10 Batch 1:  loss:  0.204375  validationAccuracy:  0.5152\n",
      "Epoch 263, CIFAR-10 Batch 1:  loss:  0.200266  validationAccuracy:  0.5124\n",
      "Epoch 264, CIFAR-10 Batch 1:  loss:  0.192852  validationAccuracy:  0.5184\n",
      "Epoch 265, CIFAR-10 Batch 1:  loss:  0.188563  validationAccuracy:  0.5158\n",
      "Epoch 266, CIFAR-10 Batch 1:  loss:  0.194652  validationAccuracy:  0.5126\n",
      "Epoch 267, CIFAR-10 Batch 1:  loss:  0.187553  validationAccuracy:  0.5146\n",
      "Epoch 268, CIFAR-10 Batch 1:  loss:  0.190461  validationAccuracy:  0.5174\n",
      "Epoch 269, CIFAR-10 Batch 1:  loss:  0.187245  validationAccuracy:  0.5182\n",
      "Epoch 270, CIFAR-10 Batch 1:  loss:  0.189066  validationAccuracy:  0.5118\n",
      "Epoch 271, CIFAR-10 Batch 1:  loss:  0.177497  validationAccuracy:  0.5202\n",
      "Epoch 272, CIFAR-10 Batch 1:  loss:  0.244548  validationAccuracy:  0.4946\n",
      "Epoch 273, CIFAR-10 Batch 1:  loss:  0.228976  validationAccuracy:  0.5044\n",
      "Epoch 274, CIFAR-10 Batch 1:  loss:  0.325734  validationAccuracy:  0.4798\n",
      "Epoch 275, CIFAR-10 Batch 1:  loss:  0.233496  validationAccuracy:  0.4986\n",
      "Epoch 276, CIFAR-10 Batch 1:  loss:  0.217789  validationAccuracy:  0.503\n",
      "Epoch 277, CIFAR-10 Batch 1:  loss:  0.233881  validationAccuracy:  0.4842\n",
      "Epoch 278, CIFAR-10 Batch 1:  loss:  0.215142  validationAccuracy:  0.4956\n",
      "Epoch 279, CIFAR-10 Batch 1:  loss:  0.201987  validationAccuracy:  0.4926\n",
      "Epoch 280, CIFAR-10 Batch 1:  loss:  0.222603  validationAccuracy:  0.49\n",
      "Epoch 281, CIFAR-10 Batch 1:  loss:  0.244924  validationAccuracy:  0.4762\n",
      "Epoch 282, CIFAR-10 Batch 1:  loss:  0.219256  validationAccuracy:  0.4862\n",
      "Epoch 283, CIFAR-10 Batch 1:  loss:  0.202677  validationAccuracy:  0.4934\n",
      "Epoch 284, CIFAR-10 Batch 1:  loss:  0.22628  validationAccuracy:  0.4804\n",
      "Epoch 285, CIFAR-10 Batch 1:  loss:  0.320096  validationAccuracy:  0.4652\n",
      "Epoch 286, CIFAR-10 Batch 1:  loss:  0.262552  validationAccuracy:  0.4792\n",
      "Epoch 287, CIFAR-10 Batch 1:  loss:  0.205533  validationAccuracy:  0.49\n",
      "Epoch 288, CIFAR-10 Batch 1:  loss:  0.251646  validationAccuracy:  0.4758\n",
      "Epoch 289, CIFAR-10 Batch 1:  loss:  0.328846  validationAccuracy:  0.465\n",
      "Epoch 290, CIFAR-10 Batch 1:  loss:  0.236639  validationAccuracy:  0.4824\n",
      "Epoch 291, CIFAR-10 Batch 1:  loss:  0.266746  validationAccuracy:  0.468\n",
      "Epoch 292, CIFAR-10 Batch 1:  loss:  0.211962  validationAccuracy:  0.5006\n",
      "Epoch 293, CIFAR-10 Batch 1:  loss:  0.286155  validationAccuracy:  0.4762\n",
      "Epoch 294, CIFAR-10 Batch 1:  loss:  0.191929  validationAccuracy:  0.5026\n",
      "Epoch 295, CIFAR-10 Batch 1:  loss:  0.224575  validationAccuracy:  0.4712\n",
      "Epoch 296, CIFAR-10 Batch 1:  loss:  0.180279  validationAccuracy:  0.5056\n",
      "Epoch 297, CIFAR-10 Batch 1:  loss:  0.185968  validationAccuracy:  0.4888\n",
      "Epoch 298, CIFAR-10 Batch 1:  loss:  0.178862  validationAccuracy:  0.5002\n",
      "Epoch 299, CIFAR-10 Batch 1:  loss:  0.173282  validationAccuracy:  0.4828\n",
      "Epoch 300, CIFAR-10 Batch 1:  loss:  0.177573  validationAccuracy:  0.49\n",
      "Epoch 301, CIFAR-10 Batch 1:  loss:  0.172626  validationAccuracy:  0.4874\n",
      "Epoch 302, CIFAR-10 Batch 1:  loss:  0.221311  validationAccuracy:  0.477\n",
      "Epoch 303, CIFAR-10 Batch 1:  loss:  0.294746  validationAccuracy:  0.4668\n",
      "Epoch 304, CIFAR-10 Batch 1:  loss:  0.272693  validationAccuracy:  0.4676\n",
      "Epoch 305, CIFAR-10 Batch 1:  loss:  0.157092  validationAccuracy:  0.498\n",
      "Epoch 306, CIFAR-10 Batch 1:  loss:  0.237109  validationAccuracy:  0.4708\n",
      "Epoch 307, CIFAR-10 Batch 1:  loss:  0.143629  validationAccuracy:  0.5006\n",
      "Epoch 308, CIFAR-10 Batch 1:  loss:  0.160638  validationAccuracy:  0.4798\n",
      "Epoch 309, CIFAR-10 Batch 1:  loss:  0.153549  validationAccuracy:  0.4986\n",
      "Epoch 310, CIFAR-10 Batch 1:  loss:  0.154282  validationAccuracy:  0.4924\n",
      "Epoch 311, CIFAR-10 Batch 1:  loss:  0.137626  validationAccuracy:  0.4886\n",
      "Epoch 312, CIFAR-10 Batch 1:  loss:  0.129644  validationAccuracy:  0.5006\n",
      "Epoch 313, CIFAR-10 Batch 1:  loss:  0.160046  validationAccuracy:  0.4814\n",
      "Epoch 314, CIFAR-10 Batch 1:  loss:  0.211635  validationAccuracy:  0.4836\n",
      "Epoch 315, CIFAR-10 Batch 1:  loss:  0.25415  validationAccuracy:  0.4608\n",
      "Epoch 316, CIFAR-10 Batch 1:  loss:  0.173276  validationAccuracy:  0.488\n",
      "Epoch 317, CIFAR-10 Batch 1:  loss:  0.220402  validationAccuracy:  0.4876\n",
      "Epoch 318, CIFAR-10 Batch 1:  loss:  0.18598  validationAccuracy:  0.4882\n",
      "Epoch 319, CIFAR-10 Batch 1:  loss:  0.212615  validationAccuracy:  0.4868\n",
      "Epoch 320, CIFAR-10 Batch 1:  loss:  0.138432  validationAccuracy:  0.4896\n",
      "Epoch 321, CIFAR-10 Batch 1:  loss:  0.177614  validationAccuracy:  0.4754\n",
      "Epoch 322, CIFAR-10 Batch 1:  loss:  0.144322  validationAccuracy:  0.4856\n",
      "Epoch 323, CIFAR-10 Batch 1:  loss:  0.158967  validationAccuracy:  0.4878\n",
      "Epoch 324, CIFAR-10 Batch 1:  loss:  0.205346  validationAccuracy:  0.504\n",
      "Epoch 325, CIFAR-10 Batch 1:  loss:  0.145064  validationAccuracy:  0.493\n",
      "Epoch 326, CIFAR-10 Batch 1:  loss:  0.100513  validationAccuracy:  0.4974\n",
      "Epoch 327, CIFAR-10 Batch 1:  loss:  0.134546  validationAccuracy:  0.4832\n",
      "Epoch 328, CIFAR-10 Batch 1:  loss:  0.130503  validationAccuracy:  0.4806\n",
      "Epoch 329, CIFAR-10 Batch 1:  loss:  0.140753  validationAccuracy:  0.4738\n",
      "Epoch 330, CIFAR-10 Batch 1:  loss:  0.113202  validationAccuracy:  0.498\n",
      "Epoch 331, CIFAR-10 Batch 1:  loss:  0.153302  validationAccuracy:  0.4686\n",
      "Epoch 332, CIFAR-10 Batch 1:  loss:  0.183204  validationAccuracy:  0.4718\n",
      "Epoch 333, CIFAR-10 Batch 1:  loss:  0.210443  validationAccuracy:  0.4572\n",
      "Epoch 334, CIFAR-10 Batch 1:  loss:  0.272203  validationAccuracy:  0.4572\n",
      "Epoch 335, CIFAR-10 Batch 1:  loss:  0.168427  validationAccuracy:  0.4714\n",
      "Epoch 336, CIFAR-10 Batch 1:  loss:  0.17134  validationAccuracy:  0.4738\n",
      "Epoch 337, CIFAR-10 Batch 1:  loss:  0.178325  validationAccuracy:  0.4608\n",
      "Epoch 338, CIFAR-10 Batch 1:  loss:  0.178596  validationAccuracy:  0.4538\n",
      "Epoch 339, CIFAR-10 Batch 1:  loss:  0.21931  validationAccuracy:  0.4586\n",
      "Epoch 340, CIFAR-10 Batch 1:  loss:  0.137634  validationAccuracy:  0.4816\n",
      "Epoch 341, CIFAR-10 Batch 1:  loss:  0.314247  validationAccuracy:  0.4516\n",
      "Epoch 342, CIFAR-10 Batch 1:  loss:  0.273052  validationAccuracy:  0.459\n",
      "Epoch 343, CIFAR-10 Batch 1:  loss:  0.187405  validationAccuracy:  0.4572\n",
      "Epoch 344, CIFAR-10 Batch 1:  loss:  0.135793  validationAccuracy:  0.4856\n",
      "Epoch 345, CIFAR-10 Batch 1:  loss:  0.162599  validationAccuracy:  0.4768\n",
      "Epoch 346, CIFAR-10 Batch 1:  loss:  0.188486  validationAccuracy:  0.4752\n",
      "Epoch 347, CIFAR-10 Batch 1:  loss:  0.152169  validationAccuracy:  0.4816\n",
      "Epoch 348, CIFAR-10 Batch 1:  loss:  0.0977199  validationAccuracy:  0.483\n",
      "Epoch 349, CIFAR-10 Batch 1:  loss:  0.0964659  validationAccuracy:  0.4914\n",
      "Epoch 350, CIFAR-10 Batch 1:  loss:  0.113658  validationAccuracy:  0.4882\n",
      "Epoch 351, CIFAR-10 Batch 1:  loss:  0.149087  validationAccuracy:  0.4854\n",
      "Epoch 352, CIFAR-10 Batch 1:  loss:  0.183143  validationAccuracy:  0.4736\n",
      "Epoch 353, CIFAR-10 Batch 1:  loss:  0.114215  validationAccuracy:  0.4932\n",
      "Epoch 354, CIFAR-10 Batch 1:  loss:  0.107376  validationAccuracy:  0.4784\n",
      "Epoch 355, CIFAR-10 Batch 1:  loss:  0.125463  validationAccuracy:  0.4814\n",
      "Epoch 356, CIFAR-10 Batch 1:  loss:  0.165439  validationAccuracy:  0.468\n",
      "Epoch 357, CIFAR-10 Batch 1:  loss:  0.130911  validationAccuracy:  0.4738\n",
      "Epoch 358, CIFAR-10 Batch 1:  loss:  0.176296  validationAccuracy:  0.4702\n",
      "Epoch 359, CIFAR-10 Batch 1:  loss:  0.191733  validationAccuracy:  0.4648\n",
      "Epoch 360, CIFAR-10 Batch 1:  loss:  0.172572  validationAccuracy:  0.4724\n",
      "Epoch 361, CIFAR-10 Batch 1:  loss:  0.178129  validationAccuracy:  0.4756\n",
      "Epoch 362, CIFAR-10 Batch 1:  loss:  0.0826467  validationAccuracy:  0.4948\n",
      "Epoch 363, CIFAR-10 Batch 1:  loss:  0.0962291  validationAccuracy:  0.481\n",
      "Epoch 364, CIFAR-10 Batch 1:  loss:  0.145585  validationAccuracy:  0.481\n",
      "Epoch 365, CIFAR-10 Batch 1:  loss:  0.117465  validationAccuracy:  0.483\n",
      "Epoch 366, CIFAR-10 Batch 1:  loss:  0.0845374  validationAccuracy:  0.4878\n",
      "Epoch 367, CIFAR-10 Batch 1:  loss:  0.0787361  validationAccuracy:  0.4784\n",
      "Epoch 368, CIFAR-10 Batch 1:  loss:  0.0656403  validationAccuracy:  0.488\n",
      "Epoch 369, CIFAR-10 Batch 1:  loss:  0.0760845  validationAccuracy:  0.4796\n",
      "Epoch 370, CIFAR-10 Batch 1:  loss:  0.0868116  validationAccuracy:  0.4858\n",
      "Epoch 371, CIFAR-10 Batch 1:  loss:  0.0959902  validationAccuracy:  0.4736\n",
      "Epoch 372, CIFAR-10 Batch 1:  loss:  0.0926972  validationAccuracy:  0.4782\n",
      "Epoch 373, CIFAR-10 Batch 1:  loss:  0.10037  validationAccuracy:  0.4864\n",
      "Epoch 374, CIFAR-10 Batch 1:  loss:  0.0739728  validationAccuracy:  0.4932\n",
      "Epoch 375, CIFAR-10 Batch 1:  loss:  0.0950581  validationAccuracy:  0.4904\n",
      "Epoch 376, CIFAR-10 Batch 1:  loss:  0.151043  validationAccuracy:  0.4764\n",
      "Epoch 377, CIFAR-10 Batch 1:  loss:  0.118156  validationAccuracy:  0.4744\n",
      "Epoch 378, CIFAR-10 Batch 1:  loss:  0.201938  validationAccuracy:  0.4712\n",
      "Epoch 379, CIFAR-10 Batch 1:  loss:  0.203879  validationAccuracy:  0.473\n",
      "Epoch 380, CIFAR-10 Batch 1:  loss:  0.139839  validationAccuracy:  0.4776\n",
      "Epoch 381, CIFAR-10 Batch 1:  loss:  0.111003  validationAccuracy:  0.4752\n",
      "Epoch 382, CIFAR-10 Batch 1:  loss:  0.225506  validationAccuracy:  0.4724\n",
      "Epoch 383, CIFAR-10 Batch 1:  loss:  0.220377  validationAccuracy:  0.4614\n",
      "Epoch 384, CIFAR-10 Batch 1:  loss:  0.18553  validationAccuracy:  0.4842\n",
      "Epoch 385, CIFAR-10 Batch 1:  loss:  0.131939  validationAccuracy:  0.4758\n",
      "Epoch 386, CIFAR-10 Batch 1:  loss:  0.0886256  validationAccuracy:  0.4884\n",
      "Epoch 387, CIFAR-10 Batch 1:  loss:  0.104605  validationAccuracy:  0.476\n",
      "Epoch 388, CIFAR-10 Batch 1:  loss:  0.119208  validationAccuracy:  0.465\n",
      "Epoch 389, CIFAR-10 Batch 1:  loss:  0.29055  validationAccuracy:  0.4404\n",
      "Epoch 390, CIFAR-10 Batch 1:  loss:  0.205328  validationAccuracy:  0.453\n",
      "Epoch 391, CIFAR-10 Batch 1:  loss:  0.18078  validationAccuracy:  0.4644\n",
      "Epoch 392, CIFAR-10 Batch 1:  loss:  0.159566  validationAccuracy:  0.4662\n",
      "Epoch 393, CIFAR-10 Batch 1:  loss:  0.0867794  validationAccuracy:  0.4832\n",
      "Epoch 394, CIFAR-10 Batch 1:  loss:  0.13527  validationAccuracy:  0.4774\n",
      "Epoch 395, CIFAR-10 Batch 1:  loss:  0.0857764  validationAccuracy:  0.4754\n",
      "Epoch 396, CIFAR-10 Batch 1:  loss:  0.117572  validationAccuracy:  0.4592\n",
      "Epoch 397, CIFAR-10 Batch 1:  loss:  0.203715  validationAccuracy:  0.4402\n",
      "Epoch 398, CIFAR-10 Batch 1:  loss:  0.162535  validationAccuracy:  0.4618\n",
      "Epoch 399, CIFAR-10 Batch 1:  loss:  0.237844  validationAccuracy:  0.4648\n",
      "Epoch 400, CIFAR-10 Batch 1:  loss:  0.283691  validationAccuracy:  0.4652\n",
      "Epoch 401, CIFAR-10 Batch 1:  loss:  0.21092  validationAccuracy:  0.4598\n",
      "Epoch 402, CIFAR-10 Batch 1:  loss:  0.16384  validationAccuracy:  0.462\n",
      "Epoch 403, CIFAR-10 Batch 1:  loss:  0.256195  validationAccuracy:  0.452\n",
      "Epoch 404, CIFAR-10 Batch 1:  loss:  0.0885052  validationAccuracy:  0.484\n",
      "Epoch 405, CIFAR-10 Batch 1:  loss:  0.0947031  validationAccuracy:  0.4782\n",
      "Epoch 406, CIFAR-10 Batch 1:  loss:  0.0901387  validationAccuracy:  0.459\n",
      "Epoch 407, CIFAR-10 Batch 1:  loss:  0.0768121  validationAccuracy:  0.4782\n",
      "Epoch 408, CIFAR-10 Batch 1:  loss:  0.0842004  validationAccuracy:  0.468\n",
      "Epoch 409, CIFAR-10 Batch 1:  loss:  0.0694433  validationAccuracy:  0.4782\n",
      "Epoch 410, CIFAR-10 Batch 1:  loss:  0.0487605  validationAccuracy:  0.4746\n",
      "Epoch 411, CIFAR-10 Batch 1:  loss:  0.057565  validationAccuracy:  0.4838\n",
      "Epoch 412, CIFAR-10 Batch 1:  loss:  0.0425129  validationAccuracy:  0.4802\n",
      "Epoch 413, CIFAR-10 Batch 1:  loss:  0.0543831  validationAccuracy:  0.4812\n",
      "Epoch 414, CIFAR-10 Batch 1:  loss:  0.0791882  validationAccuracy:  0.4702\n",
      "Epoch 415, CIFAR-10 Batch 1:  loss:  0.0409624  validationAccuracy:  0.4798\n",
      "Epoch 416, CIFAR-10 Batch 1:  loss:  0.0421264  validationAccuracy:  0.49\n",
      "Epoch 417, CIFAR-10 Batch 1:  loss:  0.0464884  validationAccuracy:  0.472\n",
      "Epoch 418, CIFAR-10 Batch 1:  loss:  0.0404672  validationAccuracy:  0.482\n",
      "Epoch 419, CIFAR-10 Batch 1:  loss:  0.0279453  validationAccuracy:  0.484\n",
      "Epoch 420, CIFAR-10 Batch 1:  loss:  0.0305156  validationAccuracy:  0.4848\n",
      "Epoch 421, CIFAR-10 Batch 1:  loss:  0.0320326  validationAccuracy:  0.4862\n",
      "Epoch 422, CIFAR-10 Batch 1:  loss:  0.0388382  validationAccuracy:  0.4828\n",
      "Epoch 423, CIFAR-10 Batch 1:  loss:  0.0607566  validationAccuracy:  0.474\n",
      "Epoch 424, CIFAR-10 Batch 1:  loss:  0.105676  validationAccuracy:  0.464\n",
      "Epoch 425, CIFAR-10 Batch 1:  loss:  0.180042  validationAccuracy:  0.4562\n",
      "Epoch 426, CIFAR-10 Batch 1:  loss:  0.148967  validationAccuracy:  0.4666\n",
      "Epoch 427, CIFAR-10 Batch 1:  loss:  0.079676  validationAccuracy:  0.467\n",
      "Epoch 428, CIFAR-10 Batch 1:  loss:  0.090573  validationAccuracy:  0.4652\n",
      "Epoch 429, CIFAR-10 Batch 1:  loss:  0.0972226  validationAccuracy:  0.4594\n",
      "Epoch 430, CIFAR-10 Batch 1:  loss:  0.0738982  validationAccuracy:  0.456\n",
      "Epoch 431, CIFAR-10 Batch 1:  loss:  0.0727509  validationAccuracy:  0.4676\n",
      "Epoch 432, CIFAR-10 Batch 1:  loss:  0.0658095  validationAccuracy:  0.4716\n",
      "Epoch 433, CIFAR-10 Batch 1:  loss:  0.0648998  validationAccuracy:  0.4654\n",
      "Epoch 434, CIFAR-10 Batch 1:  loss:  0.0884861  validationAccuracy:  0.4636\n",
      "Epoch 435, CIFAR-10 Batch 1:  loss:  0.0837377  validationAccuracy:  0.472\n",
      "Epoch 436, CIFAR-10 Batch 1:  loss:  0.0864277  validationAccuracy:  0.4566\n",
      "Epoch 437, CIFAR-10 Batch 1:  loss:  0.0666146  validationAccuracy:  0.4634\n",
      "Epoch 438, CIFAR-10 Batch 1:  loss:  0.0853558  validationAccuracy:  0.4682\n",
      "Epoch 439, CIFAR-10 Batch 1:  loss:  0.0293321  validationAccuracy:  0.4734\n",
      "Epoch 440, CIFAR-10 Batch 1:  loss:  0.0347558  validationAccuracy:  0.4816\n",
      "Epoch 441, CIFAR-10 Batch 1:  loss:  0.0425762  validationAccuracy:  0.4744\n",
      "Epoch 442, CIFAR-10 Batch 1:  loss:  0.0386348  validationAccuracy:  0.4714\n",
      "Epoch 443, CIFAR-10 Batch 1:  loss:  0.0455748  validationAccuracy:  0.4698\n",
      "Epoch 444, CIFAR-10 Batch 1:  loss:  0.0534744  validationAccuracy:  0.4656\n",
      "Epoch 445, CIFAR-10 Batch 1:  loss:  0.0660214  validationAccuracy:  0.468\n",
      "Epoch 446, CIFAR-10 Batch 1:  loss:  0.0377864  validationAccuracy:  0.4762\n",
      "Epoch 447, CIFAR-10 Batch 1:  loss:  0.0595413  validationAccuracy:  0.4786\n",
      "Epoch 448, CIFAR-10 Batch 1:  loss:  0.075805  validationAccuracy:  0.4664\n",
      "Epoch 449, CIFAR-10 Batch 1:  loss:  0.19784  validationAccuracy:  0.454\n",
      "Epoch 450, CIFAR-10 Batch 1:  loss:  0.247587  validationAccuracy:  0.4592\n",
      "Epoch 451, CIFAR-10 Batch 1:  loss:  0.274629  validationAccuracy:  0.4744\n",
      "Epoch 452, CIFAR-10 Batch 1:  loss:  0.180986  validationAccuracy:  0.476\n",
      "Epoch 453, CIFAR-10 Batch 1:  loss:  0.114328  validationAccuracy:  0.491\n",
      "Epoch 454, CIFAR-10 Batch 1:  loss:  0.112177  validationAccuracy:  0.4632\n",
      "Epoch 455, CIFAR-10 Batch 1:  loss:  0.0747869  validationAccuracy:  0.48\n",
      "Epoch 456, CIFAR-10 Batch 1:  loss:  0.0831805  validationAccuracy:  0.4662\n",
      "Epoch 457, CIFAR-10 Batch 1:  loss:  0.105666  validationAccuracy:  0.4658\n",
      "Epoch 458, CIFAR-10 Batch 1:  loss:  0.117607  validationAccuracy:  0.4604\n",
      "Epoch 459, CIFAR-10 Batch 1:  loss:  0.0932543  validationAccuracy:  0.466\n",
      "Epoch 460, CIFAR-10 Batch 1:  loss:  0.100106  validationAccuracy:  0.466\n",
      "Epoch 461, CIFAR-10 Batch 1:  loss:  0.0805378  validationAccuracy:  0.4732\n",
      "Epoch 462, CIFAR-10 Batch 1:  loss:  0.0552062  validationAccuracy:  0.4736\n",
      "Epoch 463, CIFAR-10 Batch 1:  loss:  0.0505436  validationAccuracy:  0.4748\n",
      "Epoch 464, CIFAR-10 Batch 1:  loss:  0.0482471  validationAccuracy:  0.4666\n",
      "Epoch 465, CIFAR-10 Batch 1:  loss:  0.0337567  validationAccuracy:  0.4736\n",
      "Epoch 466, CIFAR-10 Batch 1:  loss:  0.036839  validationAccuracy:  0.4692\n",
      "Epoch 467, CIFAR-10 Batch 1:  loss:  0.0548237  validationAccuracy:  0.4586\n",
      "Epoch 468, CIFAR-10 Batch 1:  loss:  0.053725  validationAccuracy:  0.4634\n",
      "Epoch 469, CIFAR-10 Batch 1:  loss:  0.0467695  validationAccuracy:  0.4644\n",
      "Epoch 470, CIFAR-10 Batch 1:  loss:  0.119511  validationAccuracy:  0.4686\n",
      "Epoch 471, CIFAR-10 Batch 1:  loss:  0.0711926  validationAccuracy:  0.465\n",
      "Epoch 472, CIFAR-10 Batch 1:  loss:  0.0635457  validationAccuracy:  0.4692\n",
      "Epoch 473, CIFAR-10 Batch 1:  loss:  0.0443261  validationAccuracy:  0.4798\n",
      "Epoch 474, CIFAR-10 Batch 1:  loss:  0.0242761  validationAccuracy:  0.4798\n",
      "Epoch 475, CIFAR-10 Batch 1:  loss:  0.0275917  validationAccuracy:  0.4688\n",
      "Epoch 476, CIFAR-10 Batch 1:  loss:  0.0230051  validationAccuracy:  0.478\n",
      "Epoch 477, CIFAR-10 Batch 1:  loss:  0.0187906  validationAccuracy:  0.4804\n",
      "Epoch 478, CIFAR-10 Batch 1:  loss:  0.0140073  validationAccuracy:  0.4778\n",
      "Epoch 479, CIFAR-10 Batch 1:  loss:  0.0159042  validationAccuracy:  0.4842\n",
      "Epoch 480, CIFAR-10 Batch 1:  loss:  0.0133442  validationAccuracy:  0.476\n",
      "Epoch 481, CIFAR-10 Batch 1:  loss:  0.0194918  validationAccuracy:  0.48\n",
      "Epoch 482, CIFAR-10 Batch 1:  loss:  0.0111374  validationAccuracy:  0.48\n",
      "Epoch 483, CIFAR-10 Batch 1:  loss:  0.0251566  validationAccuracy:  0.4778\n",
      "Epoch 484, CIFAR-10 Batch 1:  loss:  0.0195009  validationAccuracy:  0.4804\n",
      "Epoch 485, CIFAR-10 Batch 1:  loss:  0.021014  validationAccuracy:  0.4746\n",
      "Epoch 486, CIFAR-10 Batch 1:  loss:  0.0191431  validationAccuracy:  0.4714\n",
      "Epoch 487, CIFAR-10 Batch 1:  loss:  0.0329014  validationAccuracy:  0.47\n",
      "Epoch 488, CIFAR-10 Batch 1:  loss:  0.0233846  validationAccuracy:  0.4664\n",
      "Epoch 489, CIFAR-10 Batch 1:  loss:  0.0200444  validationAccuracy:  0.4692\n",
      "Epoch 490, CIFAR-10 Batch 1:  loss:  0.0302104  validationAccuracy:  0.4748\n",
      "Epoch 491, CIFAR-10 Batch 1:  loss:  0.0441401  validationAccuracy:  0.475\n",
      "Epoch 492, CIFAR-10 Batch 1:  loss:  0.0365233  validationAccuracy:  0.4754\n",
      "Epoch 493, CIFAR-10 Batch 1:  loss:  0.0250882  validationAccuracy:  0.4718\n",
      "Epoch 494, CIFAR-10 Batch 1:  loss:  0.0612104  validationAccuracy:  0.4734\n",
      "Epoch 495, CIFAR-10 Batch 1:  loss:  0.0441343  validationAccuracy:  0.4668\n",
      "Epoch 496, CIFAR-10 Batch 1:  loss:  0.0463618  validationAccuracy:  0.4598\n",
      "Epoch 497, CIFAR-10 Batch 1:  loss:  0.0939033  validationAccuracy:  0.4548\n",
      "Epoch 498, CIFAR-10 Batch 1:  loss:  0.072138  validationAccuracy:  0.4626\n",
      "Epoch 499, CIFAR-10 Batch 1:  loss:  0.0990808  validationAccuracy:  0.4546\n",
      "Epoch 500, CIFAR-10 Batch 1:  loss:  0.120384  validationAccuracy:  0.4592\n",
      "Epoch 501, CIFAR-10 Batch 1:  loss:  0.139922  validationAccuracy:  0.4594\n",
      "Epoch 502, CIFAR-10 Batch 1:  loss:  0.057538  validationAccuracy:  0.471\n",
      "Epoch 503, CIFAR-10 Batch 1:  loss:  0.0830377  validationAccuracy:  0.464\n",
      "Epoch 504, CIFAR-10 Batch 1:  loss:  0.0365828  validationAccuracy:  0.4646\n",
      "Epoch 505, CIFAR-10 Batch 1:  loss:  0.0730575  validationAccuracy:  0.4706\n",
      "Epoch 506, CIFAR-10 Batch 1:  loss:  0.0850087  validationAccuracy:  0.475\n",
      "Epoch 507, CIFAR-10 Batch 1:  loss:  0.0779388  validationAccuracy:  0.4652\n",
      "Epoch 508, CIFAR-10 Batch 1:  loss:  0.114778  validationAccuracy:  0.4642\n",
      "Epoch 509, CIFAR-10 Batch 1:  loss:  0.202428  validationAccuracy:  0.4542\n",
      "Epoch 510, CIFAR-10 Batch 1:  loss:  0.0603465  validationAccuracy:  0.469\n",
      "Epoch 511, CIFAR-10 Batch 1:  loss:  0.0750173  validationAccuracy:  0.466\n",
      "Epoch 512, CIFAR-10 Batch 1:  loss:  0.135968  validationAccuracy:  0.449\n",
      "Epoch 513, CIFAR-10 Batch 1:  loss:  0.069071  validationAccuracy:  0.4624\n",
      "Epoch 514, CIFAR-10 Batch 1:  loss:  0.085143  validationAccuracy:  0.4628\n",
      "Epoch 515, CIFAR-10 Batch 1:  loss:  0.0801022  validationAccuracy:  0.4592\n",
      "Epoch 516, CIFAR-10 Batch 1:  loss:  0.119501  validationAccuracy:  0.4564\n",
      "Epoch 517, CIFAR-10 Batch 1:  loss:  0.0780275  validationAccuracy:  0.4626\n",
      "Epoch 518, CIFAR-10 Batch 1:  loss:  0.0970758  validationAccuracy:  0.4654\n",
      "Epoch 519, CIFAR-10 Batch 1:  loss:  0.15921  validationAccuracy:  0.4482\n",
      "Epoch 520, CIFAR-10 Batch 1:  loss:  0.171516  validationAccuracy:  0.4466\n",
      "Epoch 521, CIFAR-10 Batch 1:  loss:  0.207582  validationAccuracy:  0.446\n",
      "Epoch 522, CIFAR-10 Batch 1:  loss:  0.107608  validationAccuracy:  0.4638\n",
      "Epoch 523, CIFAR-10 Batch 1:  loss:  0.0459923  validationAccuracy:  0.47\n",
      "Epoch 524, CIFAR-10 Batch 1:  loss:  0.0821018  validationAccuracy:  0.465\n",
      "Epoch 525, CIFAR-10 Batch 1:  loss:  0.0318698  validationAccuracy:  0.4714\n",
      "Epoch 526, CIFAR-10 Batch 1:  loss:  0.0265279  validationAccuracy:  0.4692\n",
      "Epoch 527, CIFAR-10 Batch 1:  loss:  0.0371524  validationAccuracy:  0.4678\n",
      "Epoch 528, CIFAR-10 Batch 1:  loss:  0.0340519  validationAccuracy:  0.4636\n",
      "Epoch 529, CIFAR-10 Batch 1:  loss:  0.0187785  validationAccuracy:  0.468\n",
      "Epoch 530, CIFAR-10 Batch 1:  loss:  0.0328232  validationAccuracy:  0.466\n",
      "Epoch 531, CIFAR-10 Batch 1:  loss:  0.0234667  validationAccuracy:  0.4664\n",
      "Epoch 532, CIFAR-10 Batch 1:  loss:  0.0484329  validationAccuracy:  0.4634\n",
      "Epoch 533, CIFAR-10 Batch 1:  loss:  0.0174735  validationAccuracy:  0.4778\n",
      "Epoch 534, CIFAR-10 Batch 1:  loss:  0.0220632  validationAccuracy:  0.4648\n",
      "Epoch 535, CIFAR-10 Batch 1:  loss:  0.0340468  validationAccuracy:  0.4732\n",
      "Epoch 536, CIFAR-10 Batch 1:  loss:  0.0165234  validationAccuracy:  0.4692\n",
      "Epoch 537, CIFAR-10 Batch 1:  loss:  0.0196812  validationAccuracy:  0.4792\n",
      "Epoch 538, CIFAR-10 Batch 1:  loss:  0.0253284  validationAccuracy:  0.4636\n",
      "Epoch 539, CIFAR-10 Batch 1:  loss:  0.0296798  validationAccuracy:  0.4684\n",
      "Epoch 540, CIFAR-10 Batch 1:  loss:  0.0252111  validationAccuracy:  0.4672\n",
      "Epoch 541, CIFAR-10 Batch 1:  loss:  0.0513516  validationAccuracy:  0.4716\n",
      "Epoch 542, CIFAR-10 Batch 1:  loss:  0.0302673  validationAccuracy:  0.4658\n",
      "Epoch 543, CIFAR-10 Batch 1:  loss:  0.0281386  validationAccuracy:  0.4698\n",
      "Epoch 544, CIFAR-10 Batch 1:  loss:  0.032654  validationAccuracy:  0.4646\n",
      "Epoch 545, CIFAR-10 Batch 1:  loss:  0.0435359  validationAccuracy:  0.469\n",
      "Epoch 546, CIFAR-10 Batch 1:  loss:  0.0345983  validationAccuracy:  0.4612\n",
      "Epoch 547, CIFAR-10 Batch 1:  loss:  0.0539727  validationAccuracy:  0.4674\n",
      "Epoch 548, CIFAR-10 Batch 1:  loss:  0.0515966  validationAccuracy:  0.4594\n",
      "Epoch 549, CIFAR-10 Batch 1:  loss:  0.0579273  validationAccuracy:  0.455\n",
      "Epoch 550, CIFAR-10 Batch 1:  loss:  0.0564587  validationAccuracy:  0.4546\n",
      "Epoch 551, CIFAR-10 Batch 1:  loss:  0.088687  validationAccuracy:  0.451\n",
      "Epoch 552, CIFAR-10 Batch 1:  loss:  0.0867697  validationAccuracy:  0.4548\n",
      "Epoch 553, CIFAR-10 Batch 1:  loss:  0.0804518  validationAccuracy:  0.4612\n",
      "Epoch 554, CIFAR-10 Batch 1:  loss:  0.0606154  validationAccuracy:  0.4738\n",
      "Epoch 555, CIFAR-10 Batch 1:  loss:  0.0649715  validationAccuracy:  0.4674\n",
      "Epoch 556, CIFAR-10 Batch 1:  loss:  0.0744509  validationAccuracy:  0.459\n",
      "Epoch 557, CIFAR-10 Batch 1:  loss:  0.0822456  validationAccuracy:  0.4408\n",
      "Epoch 558, CIFAR-10 Batch 1:  loss:  0.188315  validationAccuracy:  0.4478\n",
      "Epoch 559, CIFAR-10 Batch 1:  loss:  0.0787975  validationAccuracy:  0.4524\n",
      "Epoch 560, CIFAR-10 Batch 1:  loss:  0.105484  validationAccuracy:  0.457\n",
      "Epoch 561, CIFAR-10 Batch 1:  loss:  0.169068  validationAccuracy:  0.4318\n",
      "Epoch 562, CIFAR-10 Batch 1:  loss:  0.0903381  validationAccuracy:  0.4646\n",
      "Epoch 563, CIFAR-10 Batch 1:  loss:  0.160008  validationAccuracy:  0.4534\n",
      "Epoch 564, CIFAR-10 Batch 1:  loss:  0.16031  validationAccuracy:  0.463\n",
      "Epoch 565, CIFAR-10 Batch 1:  loss:  0.111588  validationAccuracy:  0.4594\n",
      "Epoch 566, CIFAR-10 Batch 1:  loss:  0.0458891  validationAccuracy:  0.4528\n",
      "Epoch 567, CIFAR-10 Batch 1:  loss:  0.075555  validationAccuracy:  0.4558\n",
      "Epoch 568, CIFAR-10 Batch 1:  loss:  0.1051  validationAccuracy:  0.4696\n",
      "Epoch 569, CIFAR-10 Batch 1:  loss:  0.0787898  validationAccuracy:  0.4574\n",
      "Epoch 570, CIFAR-10 Batch 1:  loss:  0.0175865  validationAccuracy:  0.4732\n",
      "Epoch 571, CIFAR-10 Batch 1:  loss:  0.0303359  validationAccuracy:  0.4638\n",
      "Epoch 572, CIFAR-10 Batch 1:  loss:  0.0454943  validationAccuracy:  0.4612\n",
      "Epoch 573, CIFAR-10 Batch 1:  loss:  0.0130592  validationAccuracy:  0.4614\n",
      "Epoch 574, CIFAR-10 Batch 1:  loss:  0.0187003  validationAccuracy:  0.4756\n",
      "Epoch 575, CIFAR-10 Batch 1:  loss:  0.0100751  validationAccuracy:  0.4682\n",
      "Epoch 576, CIFAR-10 Batch 1:  loss:  0.0129212  validationAccuracy:  0.4706\n",
      "Epoch 577, CIFAR-10 Batch 1:  loss:  0.00557178  validationAccuracy:  0.4724\n",
      "Epoch 578, CIFAR-10 Batch 1:  loss:  0.0085418  validationAccuracy:  0.4784\n",
      "Epoch 579, CIFAR-10 Batch 1:  loss:  0.00700448  validationAccuracy:  0.4754\n",
      "Epoch 580, CIFAR-10 Batch 1:  loss:  0.00707949  validationAccuracy:  0.4694\n",
      "Epoch 581, CIFAR-10 Batch 1:  loss:  0.00670319  validationAccuracy:  0.4796\n",
      "Epoch 582, CIFAR-10 Batch 1:  loss:  0.00627373  validationAccuracy:  0.47\n",
      "Epoch 583, CIFAR-10 Batch 1:  loss:  0.00421835  validationAccuracy:  0.4718\n",
      "Epoch 584, CIFAR-10 Batch 1:  loss:  0.00723613  validationAccuracy:  0.4774\n",
      "Epoch 585, CIFAR-10 Batch 1:  loss:  0.00679698  validationAccuracy:  0.4704\n",
      "Epoch 586, CIFAR-10 Batch 1:  loss:  0.012444  validationAccuracy:  0.4722\n",
      "Epoch 587, CIFAR-10 Batch 1:  loss:  0.00545174  validationAccuracy:  0.473\n",
      "Epoch 588, CIFAR-10 Batch 1:  loss:  0.00927364  validationAccuracy:  0.4722\n",
      "Epoch 589, CIFAR-10 Batch 1:  loss:  0.014281  validationAccuracy:  0.4808\n",
      "Epoch 590, CIFAR-10 Batch 1:  loss:  0.0161882  validationAccuracy:  0.4704\n",
      "Epoch 591, CIFAR-10 Batch 1:  loss:  0.00803184  validationAccuracy:  0.478\n",
      "Epoch 592, CIFAR-10 Batch 1:  loss:  0.0114015  validationAccuracy:  0.4678\n",
      "Epoch 593, CIFAR-10 Batch 1:  loss:  0.0111913  validationAccuracy:  0.483\n",
      "Epoch 594, CIFAR-10 Batch 1:  loss:  0.0152268  validationAccuracy:  0.4676\n",
      "Epoch 595, CIFAR-10 Batch 1:  loss:  0.0151522  validationAccuracy:  0.4776\n",
      "Epoch 596, CIFAR-10 Batch 1:  loss:  0.0142894  validationAccuracy:  0.471\n",
      "Epoch 597, CIFAR-10 Batch 1:  loss:  0.010884  validationAccuracy:  0.4706\n",
      "Epoch 598, CIFAR-10 Batch 1:  loss:  0.0309085  validationAccuracy:  0.47\n",
      "Epoch 599, CIFAR-10 Batch 1:  loss:  0.012945  validationAccuracy:  0.4648\n",
      "Epoch 600, CIFAR-10 Batch 1:  loss:  0.0125707  validationAccuracy:  0.4646\n",
      "Epoch 601, CIFAR-10 Batch 1:  loss:  0.0110537  validationAccuracy:  0.4616\n",
      "Epoch 602, CIFAR-10 Batch 1:  loss:  0.0134338  validationAccuracy:  0.4792\n",
      "Epoch 603, CIFAR-10 Batch 1:  loss:  0.026671  validationAccuracy:  0.4584\n",
      "Epoch 604, CIFAR-10 Batch 1:  loss:  0.0438651  validationAccuracy:  0.4598\n",
      "Epoch 605, CIFAR-10 Batch 1:  loss:  0.0389878  validationAccuracy:  0.4686\n",
      "Epoch 606, CIFAR-10 Batch 1:  loss:  0.0166636  validationAccuracy:  0.463\n",
      "Epoch 607, CIFAR-10 Batch 1:  loss:  0.0281973  validationAccuracy:  0.466\n",
      "Epoch 608, CIFAR-10 Batch 1:  loss:  0.030403  validationAccuracy:  0.4564\n",
      "Epoch 609, CIFAR-10 Batch 1:  loss:  0.0170705  validationAccuracy:  0.4724\n",
      "Epoch 610, CIFAR-10 Batch 1:  loss:  0.0356228  validationAccuracy:  0.4526\n",
      "Epoch 611, CIFAR-10 Batch 1:  loss:  0.0416035  validationAccuracy:  0.467\n",
      "Epoch 612, CIFAR-10 Batch 1:  loss:  0.0195434  validationAccuracy:  0.4626\n",
      "Epoch 613, CIFAR-10 Batch 1:  loss:  0.0763785  validationAccuracy:  0.4526\n",
      "Epoch 614, CIFAR-10 Batch 1:  "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:  114.978  validationAccuracy:  0.1532\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:  63.8459  validationAccuracy:  0.2246\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:  15.2653  validationAccuracy:  0.2056\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:  8.15469  validationAccuracy:  0.242\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:  5.44275  validationAccuracy:  0.2898\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:  4.54021  validationAccuracy:  0.2906\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:  3.64048  validationAccuracy:  0.2942\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:  3.0703  validationAccuracy:  0.3198\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:  2.72307  validationAccuracy:  0.3282\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:  2.65822  validationAccuracy:  0.3512\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:  2.72329  validationAccuracy:  0.36\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:  2.43285  validationAccuracy:  0.369\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:  2.38231  validationAccuracy:  0.3592\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:  2.19202  validationAccuracy:  0.3838\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:  2.2167  validationAccuracy:  0.3774\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:  2.27424  validationAccuracy:  0.3842\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:  2.11577  validationAccuracy:  0.3882\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:  2.10255  validationAccuracy:  0.3902\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:  1.98928  validationAccuracy:  0.4006\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:  1.98768  validationAccuracy:  0.3972\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:  2.08163  validationAccuracy:  0.4094\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:  1.92973  validationAccuracy:  0.4104\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:  1.92264  validationAccuracy:  0.4104\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:  1.8748  validationAccuracy:  0.4146\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:  1.82908  validationAccuracy:  0.417\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:  1.93107  validationAccuracy:  0.4226\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:  1.83001  validationAccuracy:  0.41\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:  1.8082  validationAccuracy:  0.4316\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:  1.77242  validationAccuracy:  0.4222\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:  1.72422  validationAccuracy:  0.426\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:  1.81386  validationAccuracy:  0.4394\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:  1.76798  validationAccuracy:  0.4084\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:  1.74027  validationAccuracy:  0.4408\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:  1.64422  validationAccuracy:  0.439\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:  1.67187  validationAccuracy:  0.426\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:  1.72626  validationAccuracy:  0.449\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:  1.71742  validationAccuracy:  0.417\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:  1.66836  validationAccuracy:  0.448\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:  1.56194  validationAccuracy:  0.4534\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:  1.61388  validationAccuracy:  0.439\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:  1.6556  validationAccuracy:  0.457\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:  1.67228  validationAccuracy:  0.4274\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:  1.58445  validationAccuracy:  0.4578\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:  1.5131  validationAccuracy:  0.4622\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:  1.5633  validationAccuracy:  0.4442\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:  1.59864  validationAccuracy:  0.465\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:  1.61112  validationAccuracy:  0.4358\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:  1.51563  validationAccuracy:  0.4656\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:  1.45858  validationAccuracy:  0.4696\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:  1.50823  validationAccuracy:  0.456\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:  1.54192  validationAccuracy:  0.4716\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:  1.55074  validationAccuracy:  0.4466\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:  1.44473  validationAccuracy:  0.4768\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:  1.40903  validationAccuracy:  0.4776\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:  1.45989  validationAccuracy:  0.4632\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:  1.48987  validationAccuracy:  0.4754\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:  1.50542  validationAccuracy:  0.4548\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:  1.39444  validationAccuracy:  0.4836\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:  1.36918  validationAccuracy:  0.4824\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:  1.43448  validationAccuracy:  0.4692\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:  1.4615  validationAccuracy:  0.4772\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:  1.47645  validationAccuracy:  0.4584\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:  1.36103  validationAccuracy:  0.4874\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:  1.3314  validationAccuracy:  0.4888\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:  1.39462  validationAccuracy:  0.4758\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:  1.41372  validationAccuracy:  0.4828\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:  1.44355  validationAccuracy:  0.4632\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:  1.32489  validationAccuracy:  0.4936\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:  1.29149  validationAccuracy:  0.495\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:  1.35847  validationAccuracy:  0.4802\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:  1.38045  validationAccuracy:  0.4846\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:  1.4035  validationAccuracy:  0.4698\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:  1.28679  validationAccuracy:  0.4978\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:  1.26222  validationAccuracy:  0.4972\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:  1.33003  validationAccuracy:  0.4852\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:  1.35894  validationAccuracy:  0.4886\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:  1.39868  validationAccuracy:  0.4678\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:  1.25999  validationAccuracy:  0.5004\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:  1.24287  validationAccuracy:  0.5004\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:  1.31058  validationAccuracy:  0.4866\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:  1.31216  validationAccuracy:  0.4974\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:  1.35824  validationAccuracy:  0.4714\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:  1.22409  validationAccuracy:  0.5056\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:  1.21246  validationAccuracy:  0.506\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:  1.26656  validationAccuracy:  0.497\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:  1.28077  validationAccuracy:  0.5074\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:  1.30346  validationAccuracy:  0.485\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:  1.1891  validationAccuracy:  0.5112\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:  1.19182  validationAccuracy:  0.5032\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9df3032ebe9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4c7cf5ceefd3>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
